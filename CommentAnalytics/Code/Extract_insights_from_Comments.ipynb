{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --force-reinstall -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "gather": {
     "logged": 1731776747438
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from io import StringIO\n",
    "import json\n",
    "import math\n",
    "from typing import Dict, List\n",
    "import logging\n",
    "\n",
    "# Third-party imports\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.storage.blob import (\n",
    "    BlobServiceClient,\n",
    "    BlobClient,\n",
    "    BlobSasPermissions,\n",
    "    generate_blob_sas,\n",
    "    ContainerClient,\n",
    ")\n",
    "from openai import AzureOpenAI\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from IPython.display import Markdown, display\n",
    "import openai\n",
    "\n",
    "# Load environment variables\n",
    "# Make sure to set all the environment variables\n",
    "load_dotenv(\"local.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Solution Approach\n",
    "![approach](../Other/Comment_Analytics_Solution_Approach.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1731768986437
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Generates a SAS token for accessing a blob in Azure Storage.\n",
    "def generate_sas_token(\n",
    "    blob_service_client: BlobServiceClient, source_blob: BlobClient\n",
    ") -> str:\n",
    "\n",
    "    # Create a SAS token that's valid for one hour, as an example\n",
    "    sas_token = generate_blob_sas(\n",
    "        account_name=blob_service_client.account_name,\n",
    "        container_name=source_blob.container_name,\n",
    "        blob_name=source_blob.blob_name,\n",
    "        account_key=blob_service_client.credential.account_key,\n",
    "        permission=BlobSasPermissions(read=True),\n",
    "        expiry=datetime.now(timezone.utc) + timedelta(hours=1),\n",
    "        start=datetime.now(timezone.utc) + timedelta(hours=-1),\n",
    "    )\n",
    "    return sas_token\n",
    "\n",
    "\n",
    "# Returns the number of tokens in a text string.\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "# Aggregates text chunks into larger chunks based on token size, without splitting the initial chunks\n",
    "def aggregate_chunks(chunks: list, chunk_tokens: int) -> list:\n",
    "    agg_chunks = []\n",
    "    current_chunk = \"\"\n",
    "    text_chunk_size = chunk_tokens * 4\n",
    "    # aggregate paragraphs\n",
    "    for chunk in chunks:\n",
    "        if len(current_chunk) + len(chunk) + 2 <= text_chunk_size:\n",
    "            # Add paragraph to the current chunk with two newlines\n",
    "            current_chunk += chunk + \"\\n\\n\"\n",
    "        else:\n",
    "            # Add the current chunk to the list and start a new chunk\n",
    "            agg_chunks.append(current_chunk.strip())\n",
    "            current_chunk = chunk + \"\\n\\n\"\n",
    "\n",
    "    # Add the last chunk if it's not empty\n",
    "    if current_chunk:\n",
    "        agg_chunks.append(current_chunk.strip())\n",
    "\n",
    "    return agg_chunks\n",
    "\n",
    "\n",
    "# In some documents, paragraphs may not be consistently separated by double newlines or other clear markers. May be in those cases leveraging NLP packages like Spacy might be helpful.\n",
    "def split_text_by_paragraphs(text):\n",
    "    delimiters = [\". \\r\\n\", \".\\r\\n\", \".  \\n\"]\n",
    "    for delimiter in delimiters:\n",
    "        try:\n",
    "            # Attempt to split the text using the current delimiter\n",
    "            split_result = text.split(delimiter)\n",
    "            if len(split_result) > 1:\n",
    "                return split_result\n",
    "        except Exception as e:\n",
    "            # If an error occurs, print the error and try the next delimiter\n",
    "            logger.error(f\"Error splitting with delimiter '{delimiter}': {e}\")\n",
    "    # If no valid split was found, return the original text as a single-element list\n",
    "    logger.info(\"Could not chunk by paragraph. Returning whole text\")\n",
    "    return [text]\n",
    "\n",
    "\n",
    "#    Splits a document into chunks based on the specified criteria.\n",
    "def split_document_into_chunks(\n",
    "    content_string: str,\n",
    "    split_type: str,\n",
    "    file_type: str,\n",
    "    chunk_tokens: int,\n",
    "    encoding_name: str,\n",
    ") -> list:\n",
    "    num_tokens = num_tokens_from_string(content_string, encoding_name)\n",
    "    logger.info(f\"Number of Tokens: {num_tokens}\")\n",
    "\n",
    "    agg_chunks = []\n",
    "    chunks = []\n",
    "    if file_type == \"csv\":\n",
    "        df = pd.read_csv(StringIO(content_string))\n",
    "        # df['combined_text'] = df['title'] + '\\n' + df['comment']\n",
    "        agg_chunks = df.apply(\n",
    "            lambda row: f\"{row['Title']}\\n{row['Comment']}\", axis=1\n",
    "        ).tolist()\n",
    "\n",
    "    else:\n",
    "        if num_tokens < chunk_tokens:\n",
    "            agg_chunks.append(content_string)\n",
    "        elif (file_type == \"pdf\") and (split_type == \"sections\"):\n",
    "            # print(\"Splitting PDF into sections\")\n",
    "            # Split the document into chunks base on markdown headers.\n",
    "            headers_to_split_on = [\n",
    "                (\"#\", \"Header 1\"),\n",
    "                (\"##\", \"Header 2\"),\n",
    "                (\"###\", \"Header 3\"),\n",
    "            ]\n",
    "            text_splitter = MarkdownHeaderTextSplitter(\n",
    "                headers_to_split_on=headers_to_split_on\n",
    "            )\n",
    "            ts_chunks = text_splitter.split_text(content_string)\n",
    "            for ts_chunk in ts_chunks:\n",
    "                chunks.append(ts_chunk.page_content)\n",
    "            agg_chunks = aggregate_chunks(chunks, chunk_tokens)\n",
    "            # logger.debug(f\"This book has {len(agg_chunks)} sections in it\")\n",
    "        elif file_type == \"text\":\n",
    "            # print(\"Splitting TXT into paragraphs\")\n",
    "            # Split the text into paragraphs. Following may need to be updated\n",
    "            paragraphs = split_text_by_paragraphs(content_string)\n",
    "            agg_chunks = aggregate_chunks(paragraphs, chunk_tokens)\n",
    "\n",
    "    return agg_chunks\n",
    "\n",
    "\n",
    "# Gets a response from the Azure OpenAI service based on the provided prompts.\n",
    "def getResponseFromAoAi(systemPrompt, userPrompt, temp=0):\n",
    "    conversaion = [\n",
    "        {\"role\": \"system\", \"content\": systemPrompt},\n",
    "        {\"role\": \"user\", \"content\": userPrompt},\n",
    "    ]\n",
    "    try:\n",
    "        # Send the conversation to the API\n",
    "        response = client.chat.completions.create(\n",
    "            model=aoai_api_deployment_name,  # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "            messages=conversaion,\n",
    "            # response_format={ \"type\": \"json_object\" }, #requires ptu enabled gpt4\n",
    "            temperature=temp,\n",
    "        )\n",
    "        responseText = response.choices[0].message.content\n",
    "        return responseText\n",
    "    except openai.APIError as error:\n",
    "        if error.code == \"content_filter\":\n",
    "            responseText = \"Content_Filter_Error\"\n",
    "        return responseText\n",
    "\n",
    "\n",
    "def clean_json_string(json_string):\n",
    "    pattern = r\"^```json\\s*(.*?)\\s*```$\"\n",
    "    cleaned_string = re.sub(pattern, r\"\\1\", json_string, flags=re.DOTALL)\n",
    "    return cleaned_string.strip()\n",
    "\n",
    "\n",
    "def insights_from_ind_comments(\n",
    "    chunks: list,\n",
    "    temperature: float,\n",
    "    response_type: str,\n",
    "    output_container_client: ContainerClient,\n",
    "    llm_response_file_name: str,\n",
    "    file_type: str,\n",
    "):\n",
    "    try:\n",
    "        if file_type == \"csv\":\n",
    "            i = 1\n",
    "            for chunk in chunks:\n",
    "                if chunk.strip():\n",
    "                    final_user_prompt = chunk_insights_usr_prompt.format(\n",
    "                        document_text=chunk\n",
    "                    )\n",
    "                    aoai_response = getResponseFromAoAi(\n",
    "                        chunk_insights_sys_prompt, final_user_prompt, temp=temperature\n",
    "                    )\n",
    "                    if aoai_response == \"Content_Filter_Error\":\n",
    "                        print(f\"Content Filter error with : {chunk}\")\n",
    "                    else:\n",
    "                        cleaned_response = clean_json_string(aoai_response)\n",
    "                        blob_client = output_container_client.get_blob_client(\n",
    "                            llm_response_file_name + str(i) + \".json\"\n",
    "                        )\n",
    "                        blob_client.upload_blob(cleaned_response, overwrite=True)\n",
    "                i += 1\n",
    "        else:\n",
    "            aggregated_results = []\n",
    "            for chunk in chunks:\n",
    "                if chunk.strip():\n",
    "                    final_user_prompt = chunk_insights_usr_prompt.format(\n",
    "                        document_text=chunk\n",
    "                    )\n",
    "                    aoai_response = getResponseFromAoAi(\n",
    "                        chunk_insights_sys_prompt, final_user_prompt, temp=temperature\n",
    "                    )\n",
    "                    if aoai_response == \"Content_Filter_Error\":\n",
    "                        print(f\"Content Filter error with : {chunk}\")\n",
    "                    else:\n",
    "                        cleaned_response = clean_json_string(aoai_response)\n",
    "                        json_response = json.loads(cleaned_response)\n",
    "                        aggregated_results.append(json_response)\n",
    "\n",
    "            aggregated_results_str = json.dumps(aggregated_results, indent=4)\n",
    "            main_themes_count = sum(\n",
    "                1 for obj in aggregated_results if \"main_themes\" in obj\n",
    "            )\n",
    "            if main_themes_count > 1:\n",
    "                agg_resp_json = aggregate_individual_insights(aggregated_results)\n",
    "                blob_client = output_container_client.get_blob_client(\n",
    "                    llm_response_file_name\n",
    "                )\n",
    "                blob_client.upload_blob(agg_resp_json, overwrite=True)\n",
    "            else:\n",
    "                blob_client = output_container_client.get_blob_client(\n",
    "                    llm_response_file_name\n",
    "                )\n",
    "                aggregated_results_str = json.dumps(aggregated_results[0], indent=4)\n",
    "                blob_client.upload_blob(aggregated_results_str, overwrite=True)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to generate individual comment insight jsons: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def aggregate_individual_insights(ind_insights_json) -> str:\n",
    "    agg_insights_json: Dict[str, List] = {\n",
    "        \"summary\": [],\n",
    "        \"main_themes\": [],\n",
    "        \"overall_sentiment\": [],\n",
    "        \"aspect_based_sentiment\": [],\n",
    "        \"suggestions\": [],\n",
    "    }\n",
    "    for obj in ind_insights_json:\n",
    "        agg_insights_json[\"summary\"].append(obj[\"summary\"].strip())\n",
    "        agg_insights_json[\"main_themes\"].extend(obj[\"main_themes\"])\n",
    "        agg_insights_json[\"overall_sentiment\"].append(obj[\"overall_sentiment\"])\n",
    "        agg_insights_json[\"aspect_based_sentiment\"].extend(\n",
    "            obj[\"aspect_based_sentiment\"]\n",
    "        )\n",
    "        suggestions = obj[\"suggestions\"]\n",
    "        if isinstance(suggestions, str):\n",
    "            agg_insights_json[\"suggestions\"].append(suggestions)\n",
    "        else:\n",
    "            agg_insights_json[\"suggestions\"].extend(suggestions)\n",
    "    agg_insights_json_str = json.dumps(agg_insights_json, indent=4)\n",
    "    return agg_insights_json_str\n",
    "\n",
    "\n",
    "def ind_comment_summary(\n",
    "    container_client: ContainerClient, temperature: float, comment_folder_name: str\n",
    "):\n",
    "    try:\n",
    "        blob_list = container_client.list_blobs(\n",
    "            name_starts_with=\"individual/\" + comment_folder_name\n",
    "        )\n",
    "        for blob in blob_list:\n",
    "            if blob.name.endswith(\".json\"):\n",
    "                comment_summary_file_name = (\n",
    "                    blob.name.replace(\"individual/\", \"individual_summary/\")\n",
    "                    .replace(\" - extracted insights\", \" - summary\")\n",
    "                    .replace(\".json\", \".md\")\n",
    "                )\n",
    "                blob_client = container_client.get_blob_client(blob.name)\n",
    "                downloaded_blob = blob_client.download_blob().readall()\n",
    "                # json_data = json.loads(downloaded_blob)\n",
    "                # ind_comments_json = json.dumps(json_data, indent=4)\n",
    "                ind_comments_json = downloaded_blob.decode(\"utf-8\")\n",
    "                final_user_prompt = comment_summary_usr_prompt.format(\n",
    "                    document_text=ind_comments_json\n",
    "                )\n",
    "                aoai_response = getResponseFromAoAi(\n",
    "                    comment_summary_sys_prompt, final_user_prompt, temp=temperature\n",
    "                )\n",
    "                blob_client = container_client.get_blob_client(\n",
    "                    comment_summary_file_name\n",
    "                )\n",
    "                blob_client.upload_blob(aoai_response, overwrite=True)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to generate individual comment summaries: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def merge_json_files(container_client: ContainerClient, comment_folder_name: str):\n",
    "    try:\n",
    "        aggregated_json: Dict[str, List] = {\n",
    "            \"summary\": [],\n",
    "            \"main_themes\": [],\n",
    "            \"suggestions\": [],\n",
    "        }\n",
    "        blob_list = container_client.list_blobs(\n",
    "            name_starts_with=\"individual/\" + comment_folder_name\n",
    "        )\n",
    "        for blob in blob_list:\n",
    "            if blob.name.endswith(\".json\"):\n",
    "                blob_client = container_client.get_blob_client(blob.name)\n",
    "                downloaded_blob = blob_client.download_blob().readall()\n",
    "                json_data = json.loads(downloaded_blob)\n",
    "                if json_data.get(\"summary\") != \"Comments in Attached File\":\n",
    "                    # Merge summary\n",
    "                    if \"summary\" in json_data:\n",
    "                        if isinstance(json_data[\"summary\"], list):\n",
    "                            aggregated_json[\"summary\"].extend(json_data[\"summary\"])\n",
    "                        else:\n",
    "                            aggregated_json[\"summary\"].append(json_data[\"summary\"])\n",
    "                    # Merge main themes\n",
    "                    if \"main_themes\" in json_data:\n",
    "                        aggregated_json[\"main_themes\"].extend(json_data[\"main_themes\"])\n",
    "                    # Merge suggestions\n",
    "                    if \"suggestions\" in json_data:\n",
    "                        if isinstance(json_data[\"suggestions\"], list):\n",
    "                            aggregated_json[\"suggestions\"].extend(\n",
    "                                json_data[\"suggestions\"]\n",
    "                            )\n",
    "                        else:\n",
    "                            aggregated_json[\"suggestions\"].append(\n",
    "                                json_data[\"suggestions\"]\n",
    "                            )\n",
    "\n",
    "        summary_json = {\"summary\": aggregated_json[\"summary\"]}\n",
    "        main_themes_json = {\"main_themes\": aggregated_json[\"main_themes\"]}\n",
    "        suggestions_json = {\"suggestions\": aggregated_json[\"suggestions\"]}\n",
    "        agg_summary_blob_client = container_client.get_blob_client(\n",
    "            \"aggregated/\" + comment_folder_name + \"/aggregated_summary.json\"\n",
    "        )\n",
    "        agg_summary_blob_client.upload_blob(\n",
    "            json.dumps(summary_json, indent=4), overwrite=True\n",
    "        )\n",
    "        agg_themes_blob_client = container_client.get_blob_client(\n",
    "            \"aggregated/\" + comment_folder_name + \"/aggregated_themes.json\"\n",
    "        )\n",
    "        agg_themes_blob_client.upload_blob(\n",
    "            json.dumps(main_themes_json, indent=4), overwrite=True\n",
    "        )\n",
    "        agg_suggestions_blob_client = container_client.get_blob_client(\n",
    "            \"aggregated/\" + comment_folder_name + \"/aggregated_suggestions.json\"\n",
    "        )\n",
    "        agg_suggestions_blob_client.upload_blob(\n",
    "            json.dumps(suggestions_json, indent=4), overwrite=True\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to generate aggregated files: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def split_agg_json_into_chunks(item_list, num_parts, item_type):\n",
    "    # Determine the length of each part\n",
    "    part_length = math.ceil(len(item_list) / num_parts)\n",
    "    # Split the item list into the specified number of parts\n",
    "    parts = [\n",
    "        item_list[i : i + part_length] for i in range(0, len(item_list), part_length)\n",
    "    ]\n",
    "    # Concatenate the items in each part\n",
    "    concatenated_parts = []\n",
    "    if item_type == \"themes\":\n",
    "        concatenated_parts = [{\"main_themes\": part} for part in parts]\n",
    "    else:\n",
    "        concatenated_parts = [\" \".join(part) for part in parts]\n",
    "    return concatenated_parts\n",
    "\n",
    "\n",
    "def aggregate_comment_summary(\n",
    "    container_client: ContainerClient, temperature: float, comment_folder_name: str\n",
    "):\n",
    "    try:\n",
    "        blob_client = container_client.get_blob_client(\n",
    "            \"aggregated/\" + comment_folder_name + \"/aggregated_summary.json\"\n",
    "        )\n",
    "        downloaded_blob = blob_client.download_blob().readall()\n",
    "        json_data = json.loads(downloaded_blob)\n",
    "        num_tokens = num_tokens_from_string(\n",
    "            \"\\n\".join(json_data[\"summary\"]), encoding_name\n",
    "        )\n",
    "        logger.info(f\"Number of Tokens for aggregated_summary: {num_tokens}\")\n",
    "        num_chunks = round(num_tokens / chunk_tokens)\n",
    "        aggregated_results = []\n",
    "        if num_chunks > 1:\n",
    "            chunks = split_agg_json_into_chunks(\n",
    "                json_data[\"summary\"], num_chunks, \"summary\"\n",
    "            )\n",
    "            # print(len(chunks))\n",
    "            for chunk in chunks:\n",
    "                final_user_prompt = agg_summary_usr_prompt.format(document_text=chunk)\n",
    "                aoai_response = getResponseFromAoAi(\n",
    "                    agg_summary_sys_prompt, final_user_prompt, temp=temperature\n",
    "                )\n",
    "                aggregated_results.append(\"Summary of Part:: \\n\" + aoai_response)\n",
    "            chunk_summary_file_name = (\n",
    "                \"aggregated_summary/\" + comment_folder_name + \"/agg_summary_chunks.md\"\n",
    "            )\n",
    "            blob_client = container_client.get_blob_client(chunk_summary_file_name)\n",
    "            chunk_responses_str = \"\\n\".join(aggregated_results)\n",
    "            blob_client.upload_blob(chunk_responses_str, overwrite=True)\n",
    "\n",
    "            final_user_prompt = agg_chunk_summary_usr_prompt.format(\n",
    "                document_text=chunk_responses_str\n",
    "            )\n",
    "            aoai_response = getResponseFromAoAi(\n",
    "                agg_chunk_summary_sys_prompt, final_user_prompt, temp=temperature\n",
    "            )\n",
    "            comment_summary_file_name = (\n",
    "                \"aggregated_summary/\" + comment_folder_name + \"/overall_summary.md\"\n",
    "            )\n",
    "            blob_client = container_client.get_blob_client(comment_summary_file_name)\n",
    "            blob_client.upload_blob(aoai_response, overwrite=True)\n",
    "        else:\n",
    "            total_summary = \"\\n\".join(json_data[\"summary\"])\n",
    "            final_user_prompt = agg_summary_usr_prompt.format(\n",
    "                document_text=total_summary\n",
    "            )\n",
    "            aoai_response = getResponseFromAoAi(\n",
    "                agg_summary_sys_prompt, final_user_prompt, temp=temperature\n",
    "            )\n",
    "            comment_summary_file_name = (\n",
    "                \"aggregated_summary/\" + comment_folder_name + \"/overall_summary.md\"\n",
    "            )\n",
    "            blob_client = container_client.get_blob_client(comment_summary_file_name)\n",
    "            blob_client.upload_blob(aoai_response, overwrite=True)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to generate aggregated summary: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def aggregate_comment_suggestions(\n",
    "    container_client: ContainerClient, temperature: float, comment_folder_name: str\n",
    "):\n",
    "    try:\n",
    "        blob_client = container_client.get_blob_client(\n",
    "            \"aggregated/\" + comment_folder_name + \"/aggregated_suggestions.json\"\n",
    "        )\n",
    "        downloaded_blob = blob_client.download_blob().readall()\n",
    "        json_data = json.loads(downloaded_blob)\n",
    "        num_tokens = num_tokens_from_string(\n",
    "            \"\\n\".join(json_data[\"suggestions\"]), encoding_name\n",
    "        )\n",
    "        logger.info(f\"Number of Tokens for aggregated_suggestions : {num_tokens}\")\n",
    "\n",
    "        num_chunks = round(num_tokens / chunk_tokens)\n",
    "        aggregated_results = []\n",
    "        if num_chunks > 1:\n",
    "            chunks = split_agg_json_into_chunks(\n",
    "                json_data[\"suggestions\"], num_chunks, \"suggestions\"\n",
    "            )\n",
    "            # print(len(chunks))\n",
    "            for chunk in chunks:\n",
    "                final_user_prompt = agg_suggestions_usr_prompt.format(\n",
    "                    document_text=chunk\n",
    "                )\n",
    "                aoai_response = getResponseFromAoAi(\n",
    "                    agg_suggestions_sys_prompt, final_user_prompt, temp=temperature\n",
    "                )\n",
    "                aggregated_results.append(\n",
    "                    \"Aggregated Suggestions Report of Part:: \\n\" + aoai_response\n",
    "                )\n",
    "            chunk_suggestions_file_name = (\n",
    "                \"aggregated_summary/\"\n",
    "                + comment_folder_name\n",
    "                + \"/agg_suggestions_chunks.md\"\n",
    "            )\n",
    "            blob_client = container_client.get_blob_client(chunk_suggestions_file_name)\n",
    "            chunk_responses_str = \"\\n\".join(aggregated_results)\n",
    "            blob_client.upload_blob(chunk_responses_str, overwrite=True)\n",
    "\n",
    "            final_user_prompt = agg_chunk_suggestions_usr_prompt.format(\n",
    "                document_text=chunk_responses_str\n",
    "            )\n",
    "            aoai_response = getResponseFromAoAi(\n",
    "                agg_chunk_suggestions_sys_prompt, final_user_prompt, temp=temperature\n",
    "            )\n",
    "            comment_suggestions_file_name = (\n",
    "                \"aggregated_summary/\" + comment_folder_name + \"/overall_suggestions.md\"\n",
    "            )\n",
    "            blob_client = container_client.get_blob_client(\n",
    "                comment_suggestions_file_name\n",
    "            )\n",
    "            blob_client.upload_blob(aoai_response, overwrite=True)\n",
    "        else:\n",
    "            total_suggestions = \"\\n\".join(json_data[\"suggestions\"])\n",
    "            final_user_prompt = agg_suggestions_usr_prompt.format(\n",
    "                document_text=total_suggestions\n",
    "            )\n",
    "            aoai_response = getResponseFromAoAi(\n",
    "                agg_suggestions_sys_prompt, final_user_prompt, temp=temperature\n",
    "            )\n",
    "            comment_suggestions_file_name = (\n",
    "                \"aggregated_summary/\" + comment_folder_name + \"/overall_suggestions.md\"\n",
    "            )\n",
    "            blob_client = container_client.get_blob_client(\n",
    "                comment_suggestions_file_name\n",
    "            )\n",
    "            blob_client.upload_blob(aoai_response, overwrite=True)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to generate aggregated summary: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def aggregate_comment_themes(\n",
    "    container_client: ContainerClient, temperature: float, comment_folder_name: str\n",
    "):\n",
    "    try:\n",
    "        blob_client = container_client.get_blob_client(\n",
    "            \"aggregated/\" + comment_folder_name + \"/aggregated_themes.json\"\n",
    "        )\n",
    "        downloaded_blob = blob_client.download_blob().readall()\n",
    "        json_data = json.loads(downloaded_blob)\n",
    "        num_tokens = num_tokens_from_string(\n",
    "            json.dumps(json_data[\"main_themes\"]), encoding_name\n",
    "        )\n",
    "        logger.info(f\"Number of Tokens for aggregated_themes: {num_tokens}\")\n",
    "        num_chunks = round(num_tokens / chunk_tokens)\n",
    "        aggregated_results = []\n",
    "        if num_chunks > 1:\n",
    "            chunks = split_agg_json_into_chunks(\n",
    "                json_data[\"main_themes\"], num_chunks, \"themes\"\n",
    "            )\n",
    "            # print(len(chunks))\n",
    "            for chunk in chunks:\n",
    "                final_user_prompt = agg_themes_usr_prompt.format(document_text=chunk)\n",
    "                aoai_response = getResponseFromAoAi(\n",
    "                    agg_themes_sys_prompt, final_user_prompt, temp=temperature\n",
    "                )\n",
    "                aggregated_results.append(re.sub(r\"```markdown\", \"\", aoai_response))\n",
    "            chunk_themes_file_name = (\n",
    "                \"aggregated_summary/\" + comment_folder_name + \"/agg_themes_chunks.md\"\n",
    "            )\n",
    "            blob_client = container_client.get_blob_client(chunk_themes_file_name)\n",
    "            chunk_responses_str = \"\\n\".join(aggregated_results)\n",
    "            blob_client.upload_blob(chunk_responses_str, overwrite=True)\n",
    "\n",
    "            final_user_prompt = agg_chunk_themes_usr_prompt.format(\n",
    "                document_text=chunk_responses_str\n",
    "            )\n",
    "            aoai_response = getResponseFromAoAi(\n",
    "                agg_chunk_themes_sys_prompt, final_user_prompt, temp=temperature\n",
    "            )\n",
    "            comment_themes_file_name = (\n",
    "                \"aggregated_summary/\" + comment_folder_name + \"/overall_themes.md\"\n",
    "            )\n",
    "            blob_client = container_client.get_blob_client(comment_themes_file_name)\n",
    "            blob_client.upload_blob(aoai_response, overwrite=True)\n",
    "        else:\n",
    "            agg_themes_json = downloaded_blob.decode(\"utf-8\")\n",
    "            final_user_prompt = agg_themes_usr_prompt.format(\n",
    "                document_text=agg_themes_json\n",
    "            )\n",
    "            aoai_response = getResponseFromAoAi(\n",
    "                agg_themes_sys_prompt, final_user_prompt, temp=temperature\n",
    "            )\n",
    "            comment_theme_file_name = (\n",
    "                \"aggregated_summary/\" + comment_folder_name + \"/overall_themes.md\"\n",
    "            )\n",
    "            blob_client = container_client.get_blob_client(comment_theme_file_name)\n",
    "            blob_client.upload_blob(aoai_response, overwrite=True)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to generate aggregated summary: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def generate_overall_summary(\n",
    "    container_client: ContainerClient, temperature: float, comment_folder_name: str\n",
    "):\n",
    "    try:\n",
    "        blob_list = container_client.list_blobs(\n",
    "            name_starts_with=\"aggregated_summary/\" + comment_folder_name\n",
    "        )\n",
    "        for blob in blob_list:\n",
    "            if blob.name.endswith(\"overall_summary.md\"):\n",
    "                blob_client = container_client.get_blob_client(blob.name)\n",
    "                downloaded_blob = blob_client.download_blob().readall()\n",
    "                summary_text = downloaded_blob.decode(\"utf-8\")\n",
    "            elif blob.name.endswith(\"overall_suggestions.md\"):\n",
    "                blob_client = container_client.get_blob_client(blob.name)\n",
    "                downloaded_blob = blob_client.download_blob().readall()\n",
    "                suggestions_text = downloaded_blob.decode(\"utf-8\")\n",
    "            elif blob.name.endswith(\"overall_themes.md\"):\n",
    "                blob_client = container_client.get_blob_client(blob.name)\n",
    "                downloaded_blob = blob_client.download_blob().readall()\n",
    "                themes_text = downloaded_blob.decode(\"utf-8\")\n",
    "\n",
    "        num_tokens = num_tokens_from_string(\n",
    "            summary_text + themes_text + suggestions_text, encoding_name\n",
    "        )\n",
    "        logger.info(f\"Number of Tokens for over all summary is: {num_tokens}\")\n",
    "\n",
    "        final_user_prompt = overall_summary_usr_prompt.format(\n",
    "            summary=summary_text, themes=themes_text, suggestions=suggestions_text\n",
    "        )\n",
    "        aoai_response = getResponseFromAoAi(\n",
    "            overall_summary_sys_prompt, final_user_prompt, temp=temperature\n",
    "        )\n",
    "        overall_summary_file_name = \"final/\" + comment_folder_name + \"/exec_summary.md\"\n",
    "        blob_client = container_client.get_blob_client(overall_summary_file_name)\n",
    "        blob_client.upload_blob(aoai_response, overwrite=True)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to generate executive summary: {e}\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1731769210175
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# prompts extracting insights from each chunk\n",
    "\n",
    "chunk_insights_sys_prompt = \"\"\" You are an AI assistant trained to analyze and summarize feedback regarding rules and regulations implemented by our organization. You will be provided with a piece of feedback from individuals or organizations, and your task is to:\n",
    "\n",
    "1. Check if the feedback text mentions that the comments or submitter information are provided in an attached file (e.g., \"See attached file(s),\" \"Please see the attached document for comments,\" \"Please find attached comments,\" etc.).\n",
    "2. If the feedback contains no other meaningful comments beyond references to attached files, do not process further and respond only with a summary saying \"Comments in Attached File.\"\n",
    "3. If the feedback includes meaningful comments in addition to references to attached files, proceed to:\n",
    "    - Extract and identify the submitter information (which may be located at the beginning or the end of the feedback text).\n",
    "    - Summarize the feedback.\n",
    "    - Identify the main theme(s) mentioned in the feedback and provide one or two sentences summarizing each theme.\n",
    "    - Determine the overall sentiment and its score (range: -1 to +1).\n",
    "    - Identify specific aspects (which will be the same as the themes) mentioned in the feedback, and determine the sentiment and score (range: -1 to +1) for each aspect.\n",
    "    - Identify any suggestions or remediations mentioned in the feedback.\n",
    "\n",
    "Provide the output in JSON format as follows:\n",
    "{\n",
    "    \"submitter_info\": \"[Submitter's name, organization, or any identifying information, if available]\",\n",
    "    \"summary\": \"[Your summary here or 'Comments in Attached File']\",\n",
    "    \"main_themes\": [\n",
    "        {\"theme\": \"Theme1\", \"summary\": \"[One or two sentences summarizing Theme1]\"},\n",
    "        {\"theme\": \"Theme2\", \"summary\": \"[One or two sentences summarizing Theme2]\"},\n",
    "        ...\n",
    "    ],\n",
    "    \"overall_sentiment\": {\n",
    "        \"sentiment\": \"[Positive/Negative/Neutral]\",\n",
    "        \"score\": [Sentiment score]\n",
    "    },\n",
    "    \"aspect_based_sentiment\": [\n",
    "        {\"aspect\": \"Theme1\", \"sentiment\": \"[Positive/Negative/Neutral]\", \"score\": [Sentiment score]},\n",
    "        {\"aspect\": \"Theme2\", \"sentiment\": \"[Positive/Negative/Neutral]\", \"score\": [Sentiment score]},\n",
    "        ...\n",
    "    ],\n",
    "    \"suggestions\": \"[Suggestions or remediations mentioned in the feedback, if any]\",\n",
    "}\n",
    "\n",
    "Here is the user feedback for your analysis:\n",
    " \"\"\"\n",
    "\n",
    "chunk_insights_usr_prompt = \"\"\"\n",
    "User Feedback:\n",
    "{document_text}\n",
    "\"\"\"\n",
    "# prompts generating summary from individual document's extracted insights\n",
    "\n",
    "comment_summary_sys_prompt = \"\"\"\n",
    "You are an advanced AI tasked with summarizing feedback and comments regarding the rules and regulations implemented by an organization. Each comment has been processed into a structured JSON format containing the summary of the comment, themes identified in the comment along with a brief summary of each theme, and suggestions provided.\n",
    "\n",
    "The JSON structure for each comment is as follows:\n",
    "\n",
    "{\n",
    "    \"summary\": \"abc\" or [\"abc\", \"def\"],\n",
    "    \"main_themes\": [\n",
    "        {\n",
    "            \"theme\": \"abc\",\n",
    "            \"summary\": \"def\"\n",
    "        }\n",
    "    ],\n",
    "    \"suggestions\": \"abc\" or [\"abc\", \"def\"]\n",
    "}\n",
    "\n",
    "Based on the provided JSON data, you need to generate an overall summary for broader and leadership consumption. The overall summary should include the following sections:\n",
    "\n",
    "1. **Overall Summary**: A brief overview of the general sentiment and key points from the collected feedback.\n",
    "2. **Identified Themes**: A list and description of the main themes that emerged from the comments, highlighting the most frequently mentioned themes.\n",
    "3. **Key Suggestions**: A summary of the most common and significant suggestions provided by the commenters.\n",
    "\n",
    "Your task is to provide the overall summary in the following format:\n",
    "\n",
    "**Overall Summary:**\n",
    "[Overall summary here]\n",
    "\n",
    "**Identified Themes:**\n",
    "- **Theme 1**: [Theme 1 summary]\n",
    "- **Theme 2**: [Theme 2 summary]\n",
    "- // Add more themes as identified\n",
    "\n",
    "**Key Suggestions:**\n",
    "- [Suggestion 1]\n",
    "- [Suggestion 2]\n",
    "- // Add more suggestions as identified\n",
    "\n",
    "Here is the comment's processed JSON:\n",
    "\"\"\"\n",
    "comment_summary_usr_prompt = \"\"\"\n",
    "comment's processed JSON:\n",
    "\n",
    "{document_text}\n",
    "\"\"\"\n",
    "\n",
    "# prompt to generate aggregated summary from each comment's insights json\n",
    "agg_summary_sys_prompt = \"\"\"\n",
    "\n",
    "You are an expert in text summarization and natural language processing. Your task is to create an overall summary from a given aggregated text of summaries. The goal is to capture all relevant insights and key points related to the comments about the organization's rules and regulations. Ensure that the final summary is concise, coherent, and captures the essence of the feedback provided. Follow these steps:\n",
    "\n",
    "1. Identify the main themes and topics discussed in the aggregated text.\n",
    "2. Highlight any recurring issues or concerns mentioned by multiple commenters.\n",
    "3. Summarize positive feedback and suggestions for improvement.\n",
    "4. Ensure that the summary is comprehensive yet concise, avoiding unnecessary details.\n",
    "\n",
    "The input will be a long text containing aggregated summaries of comments. Your output should be a well-structured summary capturing all the essential points.\n",
    "\n",
    "Please proceed with the summarization based on the following text .\n",
    "\"\"\"\n",
    "agg_summary_usr_prompt = '''\n",
    "\"\"\"\n",
    "{document_text}\n",
    "\"\"\"\n",
    "Generate well-structured summary capturing all the essential points from the text provided\n",
    "'''\n",
    "\n",
    "# prompt to generate aggregated summary from merged LLM summaries\n",
    "agg_chunk_summary_sys_prompt = \"\"\"\n",
    "You are an expert in text summarization and natural language processing. Your task is to create a single, final summary based on a series of concise summaries that each capture feedback related to the organization's rules and regulations. The goal is to integrate all relevant insights and key points from these summaries into one cohesive summary.\n",
    "Follow these steps:\n",
    "\n",
    "1. **Aggregate Content**: Merge the content of all concise summaries into one unified summary, removing any duplication or repetitive information.\n",
    "2. **Identify Core Themes**: Extract and highlight the core themes and topics that are discussed across multiple summaries.\n",
    "3. **Synthesize Information**: Combine overlapping or similar insights, ensuring that all unique aspects are retained and presented in a cohesive manner.\n",
    "4. **Prioritize Important Points**: Emphasize the most critical issues, concerns, and suggestions mentioned, while ensuring the final summary is both comprehensive and succinct.\n",
    "5. **Ensure Coherence**: Structure the final summary logically, making sure it reads smoothly and presents the information in a clear, concise manner.\n",
    "\n",
    "The input will be a series of concise summaries, each separated by \"Summary of Part::\". Your output should be a single, well-structured summary that encapsulates all the essential feedback points.\n",
    "Please proceed with the summarization based on the following text.\n",
    "\"\"\"\n",
    "agg_chunk_summary_usr_prompt = '''\n",
    "\"\"\"\n",
    "{document_text}\n",
    "\"\"\"\n",
    "Generate well-structured summary capturing all the essential points from the text provided\n",
    "'''\n",
    "\n",
    "# prompt to generate aggregated suggestions from each comment's insights json\n",
    "agg_suggestions_sys_prompt = \"\"\"\n",
    "You are an expert in text summarization and natural language processing. Your task is to create an overall summary from a given aggregated text of suggestions. The goal is to capture all relevant insights and key points related to the suggestions about the organization's rules and regulations. Ensure that the final summary is concise, coherent, and captures the essence of the suggestions provided. Follow these steps:\n",
    "\n",
    "1. Identify the main themes and topics discussed in the aggregated text of suggestions.\n",
    "2. Highlight any recurring suggestions or common themes mentioned by multiple commenters.\n",
    "3. Summarize key suggestions for improvement and any positive feedback.\n",
    "4. Ensure that the summary is comprehensive yet concise, avoiding unnecessary details.\n",
    "\n",
    "The input will be a long text containing aggregated suggestions from various comment files. Your output should be a well-structured summary capturing all the essential points.\n",
    "\n",
    "Please proceed with the summarization based on the following text.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "agg_suggestions_usr_prompt = '''\n",
    "\"\"\"\n",
    "{document_text}\n",
    "\"\"\"\n",
    "Generate well-structured summary capturing all the essential points from the text provided\n",
    "'''\n",
    "\n",
    "# prompt to generate aggregated suggestions from merged LLM suggestions\n",
    "agg_chunk_suggestions_sys_prompt = \"\"\"\n",
    "You are an expert in text summarization and natural language processing. Your task is to create a comprehensive and coherent summary of the overall suggestions insights from multiple aggregated reports. Each report starts with \"Aggregated Suggestions Report of Part:: \" and summarizes suggestions related to an organization's rules and regulations. The goal is to merge these insights into a single, well-structured summary that captures all key points and avoids redundancy. It is crucial that no important information from any individual suggestion insights report is missed in the final summary.\n",
    "Steps to follow:\n",
    "\n",
    "1. Identify Main Themes and Topics: Extract the overarching themes and topics that span across the individual suggestions insights, each of which begins with \"Aggregated Suggestions Report of Part:: \".\n",
    "2. Summarize Recurring Suggestions: Combine recurring suggestions mentioned in multiple chunks, ensuring to capture their significance without duplicating the information.\n",
    "3. Summarize Key Suggestions for Improvement: Integrate the key suggestions for improvement from all chunks, preserving the essence of each suggestion.\n",
    "4. Highlight Positive Feedback: Include any positive feedback that reflects broader sentiment across the aggregated reports.\n",
    "5. Ensure Coherence and Conciseness: Ensure the final summary is coherent, free of repetition, and concisely captures the critical insights from all chunks.\n",
    "6. Preserve Important Details: Pay special attention to ensure that no important details from any of the individual suggestion insights are omitted.\n",
    "\n",
    "The input will consist of multiple summaries, each starting with \"Aggregated Suggestions Report of Part:: \", representing aggregated suggestions insights from different parts of the feedback. Your output should be a single, well-organized summary that retains all essential points while removing any redundant information.\n",
    "Please proceed with the summarization based on the following aggregated suggestions insights.\n",
    "\"\"\"\n",
    "agg_chunk_suggestions_usr_prompt = '''\n",
    "\"\"\"\n",
    "{document_text}\n",
    "\"\"\"\n",
    "Generate well-structured summary capturing all the essential points from the text provided\n",
    "'''\n",
    "\n",
    "# prompt to generate aggregated themes from each comment's insights json\n",
    "agg_themes_sys_prompt = \"\"\"\n",
    "You are an advanced AI specialized in processing feedback and identifying key themes. You are provided with a JSON input that contains a list of themes and their corresponding summaries extracted from feedback. The JSON is structured as follows:\n",
    "\n",
    "{\n",
    "    \"main_themes\": [\n",
    "        {\n",
    "            \"theme\": \"Payment Initiation\",\n",
    "            \"summary\": \"The ABA argues that Section 1033 should not be used to mandate payment initiation to and from Regulation E accounts.\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "Your task is to:\n",
    "1. Identify and group similar themes together, even if they have slightly different wording or phrasing but share the same context.\n",
    "2. Determine the frequency of each theme (including grouped similar themes).\n",
    "3. Provide the top 25 most frequently occurring themes, along with their grouped summary and frequency count.\n",
    "4. Ensure that the summaries for each grouped theme are concise and reflect the core message.\n",
    "\n",
    "Your output should be in this Markdown format without any additional explanations, just the formatted result:\n",
    "\n",
    "Top 25 Most Occurring Themes:\n",
    "- theme1: [count] [Generated summary]\n",
    "- theme2: [count] [Generated summary]\n",
    "\n",
    "Please be precise and ensure that the themes are accurately identified and counted.\n",
    "\"\"\"\n",
    "agg_themes_usr_prompt = \"\"\"\n",
    "Following is the JSON that contain the themes and their summaries: \n",
    "{document_text}\n",
    "\"\"\"\n",
    "\n",
    "# prompt to generate aggregated themes from merged LLM themes\n",
    "agg_chunk_themes_sys_prompt = \"\"\" \n",
    "You are tasked with consolidating themes extracted from various text chunks, all contained within a large text file. The file includes multiple sections, each corresponding to a text chunk. Each section begins with \"Top 25 Most Occurring Themes\" and lists themes in the following format:\n",
    "- Theme Name: [count of occurrence] Summary of the theme\n",
    "\n",
    "Your objective is to:\n",
    "1. **Consolidate Themes:** Identify and merge similar or duplicate themes across all sections. Ensure that similar themes are combined into a single theme, and their occurrence counts are summed.\n",
    "2. **Generate Top 25 Themes:** From the consolidated list, generate the \"Top 25 Most Occurring Themes\" based on the highest total occurrence counts.\n",
    "3. **Output Format:** Provide the output in the following format:\n",
    "Top 25 Most Occurring Themes\n",
    "- Theme Name: [count of occurrence] Summary of the theme\n",
    "\n",
    "Ensure the summaries accurately reflect the combined themes, and the list is ranked according to the total count of occurrences.\n",
    "\"\"\"\n",
    "agg_chunk_themes_usr_prompt = '''\n",
    "Here is the text from the file:\n",
    "\"\"\"\n",
    "{document_text}\n",
    "\"\"\"\n",
    "Ensure the summaries accurately reflect the combined themes, and the list is ranked according to the total count of occurrences.\n",
    "'''\n",
    "\n",
    "# prompt to generate executive/overall summary from aggregates summary, themes and suggestions\n",
    "overall_summary_sys_prompt = \"\"\"\n",
    "You are a highly skilled assistant with expertise in summarizing and consolidating feedback and comments from various sources. Your task is to generate an executive summary report based on the provided text from three files. The files contain summaries of comments or feedback regarding rules and regulations implemented by an organization. The executive summary should be concise, coherent, and cover all the main themes, recurring issues, positive feedback, suggestions for improvement, and conclusions.\n",
    "\n",
    "The provided files contain the following topics:\n",
    "\n",
    "File 1: Overall Summary\n",
    "- Core Themes and Topics\n",
    "- Recurring Issues and Concerns\n",
    "- Positive Feedback and Suggestions for Improvement\n",
    "- Conclusion\n",
    "\n",
    "File 2: Overall Themes\n",
    "- Top 25 Most Occurring Themes\n",
    "\n",
    "File 3: Overall Suggestions\n",
    "- Main Themes and Topics\n",
    "- Key Suggestions for Improvement\n",
    "- Recurring Suggestions\n",
    "- Positive Feedback\n",
    "\n",
    "Please read through the text from these files and create a single, consolidated executive summary report. Ensure that the report includes all significant points and provides a clear, comprehensive overview of the feedback and suggestions.\n",
    "\n",
    "### Executive Summary Report Structure:\n",
    "1. Introduction\n",
    "2. Main Themes and Topics\n",
    "3. Recurring Issues and Concerns\n",
    "4. Positive Feedback\n",
    "5. Suggestions for Improvement\n",
    "6. Conclusion\n",
    "\n",
    "Be sure to maintain a professional tone and ensure the summary is well-organized and easy to understand.\n",
    "Below is the text from the files:\n",
    "\"\"\"\n",
    "overall_summary_usr_prompt = \"\"\"\n",
    "\n",
    "File 1: Overall Summary\n",
    "{summary}\n",
    "\n",
    "File 2: Overall Themes\n",
    "{themes}\n",
    "\n",
    "File 3: Overall Suggestions\n",
    "{suggestions}\n",
    "\n",
    "Please read through the text from these files and generate a single, consolidated executive summary report. The report should be structured as follows:\n",
    "\n",
    "1. Introduction\n",
    "2. Main Themes and Topics\n",
    "3. Recurring Issues and Concerns\n",
    "4. Positive Feedback\n",
    "5. Suggestions for Improvement\n",
    "6. Conclusion\n",
    "\n",
    "Ensure that the report is comprehensive, well-organized, and professional.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "gather": {
     "logged": 1731776760665
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "aoai_api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "aoai_api_endpoint =  os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "aoai_api_deployment_name =  os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "account_name = os.getenv(\"STORAGE_ACCOUNT_NAME\")\n",
    "account_key = os.getenv(\"STORAGE_ACCOUNT_KEY\")\n",
    "input_container_name = os.getenv(\"STORAGE_INPUT_CONTAINER_NAME\")\n",
    "connection_string = f\"DefaultEndpointsProtocol=https;AccountName={account_name};AccountKey={account_key};EndpointSuffix=core.windows.net\"\n",
    "output_container_name = os.getenv(\"STORAGE_OUTPUT_CONTAINER_NAME\")\n",
    "\n",
    "docintel_endpoint=os.getenv(\"AZURE_DOC_INTEL_ENDPOINT\")\n",
    "docintel_key= os.getenv(\"AZURE_DOC_INTEL_KEY\")\n",
    "\n",
    "aoai_api_version= '2024-06-01'\n",
    "encoding_name = 'o200k_base'\n",
    "# Change the chunk token size to fit your needs\n",
    "chunk_tokens=8000\n",
    "temp=0\n",
    "# Name of the folder that has the comments\n",
    "comment_folder_name='rule-10-21-final-2'\n",
    "\n",
    "#logger config\n",
    "logger = logging.getLogger(\"comment_analytics\")\n",
    "logger.setLevel(logging.INFO)\n",
    "console_handler = logging.StreamHandler()\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# Suppress logging from external libraries by setting the root logger level to WARNING\n",
    "# logging.getLogger().setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "gather": {
     "logged": 1731777281756
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting time: 2024-11-16 17:06:23\n",
      "Starting time: 2024-11-16 17:06:23\n",
      "Starting time: 2024-11-16 17:06:23\n",
      "Starting time: 2024-11-16 17:06:23\n",
      "Blob's Name: \trule-10-21-final-2\n",
      "Blob's Name: \trule-10-21-final-2\n",
      "Blob's Name: \trule-10-21-final-2\n",
      "Blob's Name: \trule-10-21-final-2\n",
      "Blob's Content Type: \tapplication/octet-stream\n",
      "Blob's Content Type: \tapplication/octet-stream\n",
      "Blob's Content Type: \tapplication/octet-stream\n",
      "Blob's Content Type: \tapplication/octet-stream\n",
      "Blob's Name: \trule-10-21-final-2/ABA.pdf\n",
      "Blob's Name: \trule-10-21-final-2/ABA.pdf\n",
      "Blob's Name: \trule-10-21-final-2/ABA.pdf\n",
      "Blob's Name: \trule-10-21-final-2/ABA.pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Number of Tokens: 13523\n",
      "Number of Tokens: 13523\n",
      "Number of Tokens: 13523\n",
      "Number of Tokens: 13523\n",
      "Number of chunks: 3\n",
      "Number of chunks: 3\n",
      "Number of chunks: 3\n",
      "Number of chunks: 3\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/ABA.pdf\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/ABA.pdf\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/ABA.pdf\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/ABA.pdf\n",
      "Blob's Name: \trule-10-21-final-2/American Fintech Council.txt\n",
      "Blob's Name: \trule-10-21-final-2/American Fintech Council.txt\n",
      "Blob's Name: \trule-10-21-final-2/American Fintech Council.txt\n",
      "Blob's Name: \trule-10-21-final-2/American Fintech Council.txt\n",
      "Blob's Content Type: \ttext/plain\n",
      "Blob's Content Type: \ttext/plain\n",
      "Blob's Content Type: \ttext/plain\n",
      "Blob's Content Type: \ttext/plain\n",
      "Number of Tokens: 6551\n",
      "Number of Tokens: 6551\n",
      "Number of Tokens: 6551\n",
      "Number of Tokens: 6551\n",
      "Number of chunks: 1\n",
      "Number of chunks: 1\n",
      "Number of chunks: 1\n",
      "Number of chunks: 1\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/American Fintech Council.txt\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/American Fintech Council.txt\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/American Fintech Council.txt\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/American Fintech Council.txt\n",
      "Blob's Name: \trule-10-21-final-2/BPI TCH.pdf\n",
      "Blob's Name: \trule-10-21-final-2/BPI TCH.pdf\n",
      "Blob's Name: \trule-10-21-final-2/BPI TCH.pdf\n",
      "Blob's Name: \trule-10-21-final-2/BPI TCH.pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Number of Tokens: 56947\n",
      "Number of Tokens: 56947\n",
      "Number of Tokens: 56947\n",
      "Number of Tokens: 56947\n",
      "Number of chunks: 10\n",
      "Number of chunks: 10\n",
      "Number of chunks: 10\n",
      "Number of chunks: 10\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/BPI TCH.pdf\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/BPI TCH.pdf\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/BPI TCH.pdf\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/BPI TCH.pdf\n",
      "Blob's Name: \trule-10-21-final-2/FDATA.pdf\n",
      "Blob's Name: \trule-10-21-final-2/FDATA.pdf\n",
      "Blob's Name: \trule-10-21-final-2/FDATA.pdf\n",
      "Blob's Name: \trule-10-21-final-2/FDATA.pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Number of Tokens: 16666\n",
      "Number of Tokens: 16666\n",
      "Number of Tokens: 16666\n",
      "Number of Tokens: 16666\n",
      "Number of chunks: 3\n",
      "Number of chunks: 3\n",
      "Number of chunks: 3\n",
      "Number of chunks: 3\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/FDATA.pdf\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/FDATA.pdf\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/FDATA.pdf\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/FDATA.pdf\n",
      "Blob's Name: \trule-10-21-final-2/FDX.pdf\n",
      "Blob's Name: \trule-10-21-final-2/FDX.pdf\n",
      "Blob's Name: \trule-10-21-final-2/FDX.pdf\n",
      "Blob's Name: \trule-10-21-final-2/FDX.pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Number of Tokens: 3673\n",
      "Number of Tokens: 3673\n",
      "Number of Tokens: 3673\n",
      "Number of Tokens: 3673\n",
      "Number of chunks: 1\n",
      "Number of chunks: 1\n",
      "Number of chunks: 1\n",
      "Number of chunks: 1\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/FDX.pdf\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/FDX.pdf\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/FDX.pdf\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/FDX.pdf\n",
      "Blob's Name: \trule-10-21-final-2/Financial Technology Association.pdf\n",
      "Blob's Name: \trule-10-21-final-2/Financial Technology Association.pdf\n",
      "Blob's Name: \trule-10-21-final-2/Financial Technology Association.pdf\n",
      "Blob's Name: \trule-10-21-final-2/Financial Technology Association.pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Number of Tokens: 10518\n",
      "Number of Tokens: 10518\n",
      "Number of Tokens: 10518\n",
      "Number of Tokens: 10518\n",
      "Number of chunks: 2\n",
      "Number of chunks: 2\n",
      "Number of chunks: 2\n",
      "Number of chunks: 2\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/Financial Technology Association.pdf\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/Financial Technology Association.pdf\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/Financial Technology Association.pdf\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/Financial Technology Association.pdf\n",
      "Blob's Name: \trule-10-21-final-2/ICBA.pdf\n",
      "Blob's Name: \trule-10-21-final-2/ICBA.pdf\n",
      "Blob's Name: \trule-10-21-final-2/ICBA.pdf\n",
      "Blob's Name: \trule-10-21-final-2/ICBA.pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Number of Tokens: 8410\n",
      "Number of Tokens: 8410\n",
      "Number of Tokens: 8410\n",
      "Number of Tokens: 8410\n",
      "Number of chunks: 2\n",
      "Number of chunks: 2\n",
      "Number of chunks: 2\n",
      "Number of chunks: 2\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/ICBA.pdf\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/ICBA.pdf\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/ICBA.pdf\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/ICBA.pdf\n",
      "Blob's Name: \trule-10-21-final-2/JPMC.pdf\n",
      "Blob's Name: \trule-10-21-final-2/JPMC.pdf\n",
      "Blob's Name: \trule-10-21-final-2/JPMC.pdf\n",
      "Blob's Name: \trule-10-21-final-2/JPMC.pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Number of Tokens: 34893\n",
      "Number of Tokens: 34893\n",
      "Number of Tokens: 34893\n",
      "Number of Tokens: 34893\n",
      "Number of chunks: 7\n",
      "Number of chunks: 7\n",
      "Number of chunks: 7\n",
      "Number of chunks: 7\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/JPMC.pdf\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/JPMC.pdf\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/JPMC.pdf\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/JPMC.pdf\n",
      "Blob's Name: \trule-10-21-final-2/Mastercard Finicity.pdf\n",
      "Blob's Name: \trule-10-21-final-2/Mastercard Finicity.pdf\n",
      "Blob's Name: \trule-10-21-final-2/Mastercard Finicity.pdf\n",
      "Blob's Name: \trule-10-21-final-2/Mastercard Finicity.pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Number of Tokens: 7792\n",
      "Number of Tokens: 7792\n",
      "Number of Tokens: 7792\n",
      "Number of Tokens: 7792\n",
      "Number of chunks: 1\n",
      "Number of chunks: 1\n",
      "Number of chunks: 1\n",
      "Number of chunks: 1\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/Mastercard Finicity.pdf\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/Mastercard Finicity.pdf\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/Mastercard Finicity.pdf\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/Mastercard Finicity.pdf\n",
      "Blob's Name: \trule-10-21-final-2/Plaid.pdf\n",
      "Blob's Name: \trule-10-21-final-2/Plaid.pdf\n",
      "Blob's Name: \trule-10-21-final-2/Plaid.pdf\n",
      "Blob's Name: \trule-10-21-final-2/Plaid.pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Blob's Content Type: \tapplication/pdf\n",
      "Number of Tokens: 49985\n",
      "Number of Tokens: 49985\n",
      "Number of Tokens: 49985\n",
      "Number of Tokens: 49985\n",
      "Number of chunks: 10\n",
      "Number of chunks: 10\n",
      "Number of chunks: 10\n",
      "Number of chunks: 10\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/Plaid.pdf\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/Plaid.pdf\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/Plaid.pdf\n",
      "Successfully generated individual comment insight jsons for blob: \trule-10-21-final-2/Plaid.pdf\n",
      "Successfully generated individual comment summaries\n",
      "Successfully generated individual comment summaries\n",
      "Successfully generated individual comment summaries\n",
      "Successfully generated individual comment summaries\n",
      "Successfully Merged insights from individual comment files\n",
      "Successfully Merged insights from individual comment files\n",
      "Successfully Merged insights from individual comment files\n",
      "Successfully Merged insights from individual comment files\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    start_time = datetime.now()\n",
    "    logger.info(f\"Starting time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    # Create a BlobServiceClient object using the connection string\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "    # Create a ContainerClient object\n",
    "    input_container_client = blob_service_client.get_container_client(\n",
    "        input_container_name\n",
    "    )\n",
    "    output_container_client = blob_service_client.get_container_client(\n",
    "        output_container_name\n",
    "    )\n",
    "\n",
    "    # define openai and document intelligence Clients\n",
    "    client = AzureOpenAI(\n",
    "        api_key=aoai_api_key,\n",
    "        api_version=aoai_api_version,\n",
    "        azure_endpoint=aoai_api_endpoint,\n",
    "    )\n",
    "    document_intelligence_client = DocumentIntelligenceClient(\n",
    "        endpoint=docintel_endpoint, credential=AzureKeyCredential(docintel_key)\n",
    "    )\n",
    "\n",
    "    # List the blobs in the container\n",
    "    blob_list = input_container_client.list_blobs(name_starts_with=comment_folder_name)\n",
    "    chunks = []\n",
    "\n",
    "    for blob in blob_list:\n",
    "        logger.info(f\"Blob's Name: \\t{blob.name}\")\n",
    "        logger.info(f\"Blob's Content Type: \\t{blob.content_settings.content_type}\")\n",
    "\n",
    "        blob_client = input_container_client.get_blob_client(blob.name)\n",
    "        sas_token = generate_sas_token(\n",
    "            blob_service_client=blob_service_client, source_blob=blob_client\n",
    "        )\n",
    "        source_blob_sas_url = blob_client.url + \"?\" + sas_token\n",
    "        llm_response_file_name = \"\"\n",
    "        file_type = \"\"\n",
    "        if blob.content_settings.content_type == \"application/pdf\":\n",
    "            file_type = \"pdf\"\n",
    "            llm_response_file_name = \"individual/\" + blob.name.replace(\n",
    "                \".pdf\", \" - extracted insights.json\"\n",
    "            )\n",
    "            loader = AzureAIDocumentIntelligenceLoader(\n",
    "                url_path=source_blob_sas_url,\n",
    "                api_key=docintel_key,\n",
    "                api_endpoint=docintel_endpoint,\n",
    "                api_model=\"prebuilt-layout\",\n",
    "            )\n",
    "            docs = loader.load()\n",
    "            docs_string = docs[0].page_content\n",
    "            chunks = split_document_into_chunks(\n",
    "                docs_string,\n",
    "                \"sections\",\n",
    "                file_type,\n",
    "                chunk_tokens,\n",
    "                encoding_name=\"o200k_base\",\n",
    "            )\n",
    "            logger.info(f\"Number of chunks: {len(chunks)}\")\n",
    "        elif blob.content_settings.content_type == \"text/plain\":\n",
    "            file_type = \"text\"\n",
    "            llm_response_file_name = \"individual/\" + blob.name.replace(\n",
    "                \".txt\", \" - extracted insights.json\"\n",
    "            )\n",
    "            blob_data = blob_client.download_blob().readall()\n",
    "            # Attempt to decode the bytes with utf-8, fallback to other encodings if it fails\n",
    "            try:\n",
    "                text_data = blob_data.decode(\"utf-8\")\n",
    "            except UnicodeDecodeError:\n",
    "                try:\n",
    "                    text_data = blob_data.decode(\"ISO-8859-1\")\n",
    "                except UnicodeDecodeError:\n",
    "                    text_data = blob_data.decode(\"windows-1252\")\n",
    "            chunks = split_document_into_chunks(\n",
    "                text_data,\n",
    "                \"paragraphs\",\n",
    "                file_type,\n",
    "                chunk_tokens,\n",
    "                encoding_name=\"o200k_base\",\n",
    "            )\n",
    "            logger.info(f\"Number of chunks: {len(chunks)}\")\n",
    "        elif blob.content_settings.content_type in [\n",
    "            \"text/csv\",\n",
    "            \"application/vnd.ms-excel\",\n",
    "        ]:\n",
    "            file_type = \"csv\"\n",
    "            llm_response_file_name = \"individual/\" + blob.name.replace(\n",
    "                \".csv\", \" - extracted insights\"\n",
    "            )\n",
    "            blob_data = blob_client.download_blob().readall()\n",
    "            # Attempt to decode the bytes with utf-8, fallback to other encodings if it fails\n",
    "            try:\n",
    "                text_data = blob_data.decode(\"utf-8\")\n",
    "            except UnicodeDecodeError:\n",
    "                try:\n",
    "                    text_data = blob_data.decode(\"ISO-8859-1\")\n",
    "                except UnicodeDecodeError:\n",
    "                    text_data = blob_data.decode(\"windows-1252\")\n",
    "\n",
    "            chunks = split_document_into_chunks(\n",
    "                text_data, \"row\", file_type, chunk_tokens, encoding_name=\"o200k_base\"\n",
    "            )\n",
    "            logger.info(f\"Number of chunks: {len(chunks)}\")\n",
    "\n",
    "        if len(chunks) > 0:\n",
    "            # generate insights from individual comments file\n",
    "            if insights_from_ind_comments(\n",
    "                chunks,\n",
    "                temp,\n",
    "                \"json\",\n",
    "                output_container_client,\n",
    "                llm_response_file_name,\n",
    "                file_type,\n",
    "            ):\n",
    "                logger.info(\n",
    "                    f\"Successfully generated individual comment insight jsons for blob: \\t{blob.name}\"\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                logger.info(\n",
    "                    f\"Failed to generate individual comment insight jsonsfor blob: \\t{blob.name}\"\n",
    "                )\n",
    "\n",
    "    # Task:generate summary from individual comment insights json\n",
    "    if ind_comment_summary(output_container_client, temp, comment_folder_name):\n",
    "        logger.info(\"Successfully generated individual comment summaries\")\n",
    "    else:\n",
    "        logger.info(\"Failed to generate individual comment summaries\")\n",
    "\n",
    "    # Task:Merge summary/themes/suggestions from individual comment insights json\n",
    "\n",
    "    if merge_json_files(output_container_client, comment_folder_name):\n",
    "        logger.info(\"Successfully Merged insights from individual comment files\")\n",
    "    else:\n",
    "        logger.info(\"Failed to merge insights from individual comment files\")\n",
    "\n",
    "except Exception as ex:\n",
    "    logger.error(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "gather": {
     "logged": 1731777314172
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of Tokens for aggregated_summary: 3228\n",
      "Number of Tokens for aggregated_summary: 3228\n",
      "Number of Tokens for aggregated_summary: 3228\n",
      "Number of Tokens for aggregated_summary: 3228\n",
      "Successfully generated aggregated summary\n",
      "Successfully generated aggregated summary\n",
      "Successfully generated aggregated summary\n",
      "Successfully generated aggregated summary\n",
      "Number of Tokens for aggregated_suggestions : 2296\n",
      "Number of Tokens for aggregated_suggestions : 2296\n",
      "Number of Tokens for aggregated_suggestions : 2296\n",
      "Number of Tokens for aggregated_suggestions : 2296\n",
      "Successfully generated aggregated suggestions\n",
      "Successfully generated aggregated suggestions\n",
      "Successfully generated aggregated suggestions\n",
      "Successfully generated aggregated suggestions\n",
      "Number of Tokens for aggregated_themes: 10420\n",
      "Number of Tokens for aggregated_themes: 10420\n",
      "Number of Tokens for aggregated_themes: 10420\n",
      "Number of Tokens for aggregated_themes: 10420\n",
      "Successfully generated aggregated themes\n",
      "Successfully generated aggregated themes\n",
      "Successfully generated aggregated themes\n",
      "Successfully generated aggregated themes\n",
      "Number of Tokens for over all summary is: 2393\n",
      "Number of Tokens for over all summary is: 2393\n",
      "Number of Tokens for over all summary is: 2393\n",
      "Number of Tokens for over all summary is: 2393\n",
      "Successfully generated executive summary \n",
      "Successfully generated executive summary \n",
      "Successfully generated executive summary \n",
      "Successfully generated executive summary \n",
      "Total time taken: 0:08:49.505847\n",
      "Total time taken: 0:08:49.505847\n",
      "Total time taken: 0:08:49.505847\n",
      "Total time taken: 0:08:49.505847\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    # Task:Consilidated summary from aggregated summary json\n",
    "    if aggregate_comment_summary(output_container_client, temp, comment_folder_name):\n",
    "        logger.info(\"Successfully generated aggregated summary\")\n",
    "    else:\n",
    "        logger.info(\"Failed to generate aggregated summary\")\n",
    "\n",
    "    # Task:Consilidated suggestions from aggregated suggestions json\n",
    "    if aggregate_comment_suggestions(\n",
    "        output_container_client, temp, comment_folder_name\n",
    "    ):\n",
    "        logger.info(\"Successfully generated aggregated suggestions\")\n",
    "    else:\n",
    "        logger.info(\"Failed to generate aggregated suggestions\")\n",
    "\n",
    "    # Task:Consilidated themes from aggregated themes json\n",
    "    if aggregate_comment_themes(output_container_client, temp, comment_folder_name):\n",
    "        logger.info(\"Successfully generated aggregated themes\")\n",
    "    else:\n",
    "        logger.info(\"Failed to generate aggregated themes\")\n",
    "\n",
    "    # Task:Overall Summary report\n",
    "    if generate_overall_summary(output_container_client, temp, comment_folder_name):\n",
    "        logger.info(\"Successfully generated executive summary \")\n",
    "    else:\n",
    "        logger.info(\"Failed to generate executive summary \")\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    elapsed_time = end_time - start_time\n",
    "    logger.info(f\"Total time taken: {elapsed_time}\")\n",
    "\n",
    "except Exception as ex:\n",
    "    logger.error(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "gather": {
     "logged": 1731777315840
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Overall Summary:**\n",
       "The Financial Data Exchange, LLC (FDX) has provided positive feedback on the CFPB's Notice of Proposed Rulemaking for Personal Financial Data Rights. FDX emphasizes the importance of clear, interoperable standards and seeks recognition as a Qualified Industry Standards Setting Body (QSSB). They highlight their role in developing the FDX API for secure, user-permissioned financial data sharing and their commitment to fair, open, and inclusive governance.\n",
       "\n",
       "**Identified Themes:**\n",
       "- **Recognition as QSSB**: FDX seeks to be recognized as a Qualified Industry Standards Setting Body to unify the financial services industry around common, interoperable standards for user-permissioned data sharing.\n",
       "- **FDX API and Standards**: FDX highlights the success and continuous improvement of the FDX API, which has transitioned from credential-based 'screen scraping' to secure token-based access, with over 65 million consumer accounts using the API.\n",
       "- **Collaboration with CFPB**: FDX appreciates the CFPB's efforts and ongoing dialogue with industry stakeholders, seeking further clarifications and expressing a commitment to align with the CFPB's principles of fair, open, and inclusive governance.\n",
       "- **Standard Setting Body Characteristics**: FDX outlines the attributes of a standard setting body as recognized by the CFPB, including openness, balance, due process, appeals process, consensus, and transparency, and commits to embodying these attributes.\n",
       "- **Future Capabilities and Certification**: FDX discusses its current and future capabilities, including the implementation of a technical certification program to ensure compliance with standardized data formatting and the potential use of a registry for data connections.\n",
       "\n",
       "**Key Suggestions:**\n",
       "- Clarify the process and procedures for recognizing QSSBs and the standards they issue.\n",
       "- Emphasize the importance of timely recognition to ensure compliance with the Proposed Rules.\n",
       "- Provide further clarification on the definition of 'Terms and Conditions' for standardized data formatting."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#display one individual comment summary\n",
    "from IPython.display import Markdown, display\n",
    "#blob_client = output_container_client.get_blob_client( )\n",
    "blob_client = output_container_client.get_blob_client('individual_summary/'+comment_folder_name+'/FDX - summary.md')\n",
    "downloaded_blob = blob_client.download_blob().readall()\n",
    "markdown_text = downloaded_blob.decode('utf-8')\n",
    "display(Markdown(markdown_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "gather": {
     "logged": 1731777317922
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The feedback on the Consumer Financial Protection Bureau's (CFPB) Proposed Rule on Personal Financial Data Rights, primarily from financial institutions and industry associations, highlights several key themes and concerns:\n",
       "\n",
       "1. **General Support and Appreciation**: Many organizations, including the American Bankers Association (ABA), American Fintech Council (AFC), Bank Policy Institute, Clearing House Association, Financial Technology Association (FTA), and others, appreciate the CFPB's efforts to enhance consumer financial data rights and support the rule's intent to promote innovation, competition, and consumer protection.\n",
       "\n",
       "2. **Concerns and Recommendations**:\n",
       "   - **Scope and Definitions**: There are significant concerns about the rule's scope, definitions, and limitations on data use. Organizations like AFC and FTA suggest refining these aspects to avoid unintended consequences and ensure clarity.\n",
       "   - **Liability and Compliance**: Multiple commenters, including ABA and JPMorgan Chase & Co. (JPMC), emphasize the need for clear liability frameworks and practical compliance timelines. They stress the importance of fair apportionment of liability within the financial data sharing ecosystem.\n",
       "   - **Data Security and Standards**: The necessity for minimum data security standards, standardized data formats, and clear regulatory requirements is a recurring theme. Organizations advocate for industry-led standards and tokenized account numbers to enhance security.\n",
       "   - **Consumer Authorization and Authentication**: There is a strong emphasis on the need for clear distinctions between identity authentication and data sharing authorization. Commenters suggest that data providers should obtain their own consumer authorizations and manage risks associated with data sharing.\n",
       "   - **Prohibition on Fees**: Several organizations, including JPMC and FDATA North America, argue against the prohibition on fees for data access, citing substantial costs and the need for fair compensation for data providers.\n",
       "   - **Screen Scraping**: Many commenters support prohibiting screen scraping once API access is available, highlighting the risks associated with unsafe data access practices.\n",
       "   - **Implementation and Compliance Burdens**: Concerns about the technological and financial burdens on small banks and community banks are raised, with suggestions for exemptions or fee allowances to mitigate these challenges.\n",
       "\n",
       "3. **Positive Feedback and Suggestions for Improvement**:\n",
       "   - **Consumer Empowerment and Innovation**: Organizations like FTA and Plaid emphasize the importance of consumer control over financial data and the role of data access platforms in promoting competition and innovation.\n",
       "   - **Standardization and Interoperability**: There is broad support for standardized data formats and interoperable standards to facilitate secure and efficient data sharing.\n",
       "   - **Regulatory Clarity and Safe Harbors**: Commenters highlight the need for clear regulatory requirements, safe harbors for compliance, and practical approaches to authorization and data security.\n",
       "   - **Collaboration and Supervision**: Suggestions for collaboration with other regulatory bodies and direct supervision of data access platforms are made to ensure effective implementation and enforcement.\n",
       "\n",
       "4. **Specific Concerns and Detailed Feedback**:\n",
       "   - **Data Privacy Protections**: Plaid and other organizations emphasize the importance of consistent application of privacy protections to both third parties and data providers, arguing that de-identified data should not be subject to privacy restrictions.\n",
       "   - **Performance Specifications**: Concerns about the quantitative performance specifications, such as the 99.5% successful return rate, are raised, with suggestions for more practical and achievable standards.\n",
       "   - **Consumer Data Rights and Revocation**: Feedback supports the proposal to allow consumers to manage their data sharing but raises concerns about potential anticompetitive behavior and consumer confusion. Recommendations include establishing guardrails and clear authorization procedures.\n",
       "\n",
       "Overall, the feedback underscores the need for a balanced approach that ensures consumer protection, promotes innovation, and provides clear and practical regulatory requirements. The commenters advocate for refinements to the proposed rule to address their concerns and enhance its effectiveness."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#display one Aggregated summary of individual comment summaries\n",
    "\n",
    "blob_client = output_container_client.get_blob_client('aggregated_summary/'+comment_folder_name+'/overall_summary.md')\n",
    "downloaded_blob = blob_client.download_blob().readall()\n",
    "markdown_text = downloaded_blob.decode('utf-8')\n",
    "display(Markdown(markdown_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "gather": {
     "logged": 1731777320497
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The aggregated suggestions regarding the organization's rules and regulations primarily focus on the following key themes and topics:\n",
       "\n",
       "1. **Extension and Flexibility in Compliance Timelines**:\n",
       "   - Multiple commenters suggest extending the comment period and compliance timelines, particularly for small banks and large institutions, to ensure adequate preparation and implementation.\n",
       "\n",
       "2. **Clarification of Roles and Definitions**:\n",
       "   - There is a call for clear definitions of data providers, data aggregators, and third parties, as well as their respective roles and responsibilities. This includes clarifying liability rules and the scope of data collection and sharing.\n",
       "\n",
       "3. **Prohibition of Screen Scraping and Credential-Based Access**:\n",
       "   - A recurring suggestion is to explicitly prohibit screen scraping and credential-based access by third parties once API access is available, to enhance data security and privacy.\n",
       "\n",
       "4. **Data Security and Privacy Standards**:\n",
       "   - Commenters emphasize the need for robust data security standards, including minimum data security requirements, tokenized account numbers, and clear data deletion and retention policies. There is also a call for standardized data formats and machine-readable data fields.\n",
       "\n",
       "5. **Supervision and Certification of Third Parties**:\n",
       "   - Suggestions include creating a supervisory program for data aggregators, establishing a non-binding accreditation body for nonbanks, and requiring third parties to become authorized and certified. There is also a recommendation for direct CFPB supervision of these entities.\n",
       "\n",
       "6. **Consumer Control and Transparency**:\n",
       "   - Enhancing consumer control over their data is a key theme, with suggestions for opt-in/opt-out choices for data use, clear revocation methods, and transparency in data access and usage. There is also a call for model forms and guidance for small entities.\n",
       "\n",
       "7. **Fee Structures and Cost Recovery**:\n",
       "   - Commenters suggest allowing data providers to charge reasonable fees for data access and developer interfaces, and to recover costs and margins. There is also a recommendation to remove the prohibition on fees.\n",
       "\n",
       "8. **Performance and Reporting Standards**:\n",
       "   - Recommendations include adopting commercially reasonable performance specifications, modifying latency thresholds, and allowing industry bodies to publish performance statistics. There is also a call for clear public disclosure requirements for developer interface documentation.\n",
       "\n",
       "9. **Regulatory Coordination and Avoidance of Duplication**:\n",
       "   - Commenters urge the CFPB to coordinate with other regulatory bodies, avoid simultaneous rulemaking activities, and ensure new rules do not duplicate existing regulatory obligations.\n",
       "\n",
       "10. **Secondary Data Use and Data Sharing Limitations**:\n",
       "    - There are suggestions to allow responsible secondary use of data with appropriate safeguards, exempt de-identified data from restrictions, and establish stringent limitations on secondary data sharing.\n",
       "\n",
       "11. **Feedback on Specific Provisions**:\n",
       "    - Specific feedback includes withdrawing proposed  1033.211(c), ensuring Regulation E obligations for data aggregators, supporting the use of tokenized account numbers, and recognizing industry-led standards for data sharing.\n",
       "\n",
       "12. **Positive Feedback**:\n",
       "    - Some commenters appreciate the CFPB's efforts to enhance data privacy and security, and the inclusion of consumer protection measures in the proposed rules.\n",
       "\n",
       "Overall, the suggestions emphasize the need for clear definitions and roles, robust data security and privacy standards, consumer control and transparency, reasonable compliance timelines, and coordination with other regulatory bodies to avoid duplication and ensure effective implementation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#display one Aggregated suggestions of individual comment suggestions\n",
    "blob_client = output_container_client.get_blob_client('aggregated_summary/'+comment_folder_name+'/overall_suggestions.md')\n",
    "downloaded_blob = blob_client.download_blob().readall()\n",
    "markdown_text = downloaded_blob.decode('utf-8')\n",
    "display(Markdown(markdown_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "gather": {
     "logged": 1731777321251
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Top 25 Most Occurring Themes:\n",
       "- Prohibition on Fees: [4] ABA and JPMC argue against the prohibition on fees for data providers, stating it is not supported by law and ignores the real costs of building and maintaining interfaces.\n",
       "- Liability: [3] ABA and JPMC express concerns about liability in the NPRM, suggesting that liability should flow with the data and be fairly apportioned within the financial data sharing ecosystem.\n",
       "- Screen Scraping: [3] ABA, ICBA, and JPMC support the prohibition of screen scraping and credential-based access, recommending explicit provisions to prohibit unauthorized data access.\n",
       "- Consumer Authorization: [3] ABA and JPMC emphasize the importance of data providers obtaining their own consumer authorizations before sharing data with third parties to ensure informed consent and protect against allegations of improper data sharing.\n",
       "- Compliance Timelines: [3] ABA, JPMC, and Plaid argue that the proposed compliance timelines are insufficient and suggest extending the first compliance date to 24 months after the final rule's publication.\n",
       "- Data Security Standards: [2] ABA and JPMC stress that all entities in the data sharing ecosystem should meet minimum data security standards to ensure a high level of consumer data protection.\n",
       "- Standardized Data Formats: [2] ABA and ICBA support the adoption of standardized data formats throughout the ecosystem to promote competition and reduce switching costs for third parties.\n",
       "- Risk Management: [2] ABA and JPMC highlight the importance of allowing data providers to conduct comprehensive risk management programs and not be constrained by narrow definitions of risk management concerns.\n",
       "- Consumer Data Protection: [2] ABA and Plaid emphasize that existing regulations already provide sufficient protection for consumer data, and the proposed rules do not offer additional benefits.\n",
       "- Data Aggregators: [2] ABA and JPMC underscore the critical role of data aggregators in the data sharing ecosystem and call for clear regulatory obligations and oversight for these entities.\n",
       "- Consumer Data Access and Security: [1] The ABA supports secure and transparent consumer access to financial information but emphasizes that all participants in the data sharing ecosystem should adhere to high standards similar to banks.\n",
       "- Data Fields and Legal Authority: [1] The ABA contends that certain data fields in the NPRM exceed the statutory authority, such as 'authorized but not yet settled' transactions and 'upcoming bill information.'\n",
       "- Role of Data Providers and Aggregators: [1] The ABA urges the CFPB to clarify that data providers are not responsible for ensuring third parties' compliance and to prohibit screen scraping.\n",
       "- Risk Management and Fraud Prevention: [1] The ABA stresses the importance of risk management and fraud prevention, advocating for flexibility for data providers to manage risks and prevent fraud.\n",
       "- Accreditation and Compliance: [1] The ABA suggests creating a non-binding accreditation body for nonbanks to meet minimum standards, which could expedite the due diligence process for data providers.\n",
       "- Strict Construction of Section 1033: [1] ABA argues that the CFPB should limit the rule to facilitating access to consumer information and not extend it to enable transactions.\n",
       "- Competitive Impacts on Small Banks: [1] ABA highlights the potential negative impacts on small banks and urges the CFPB to consider the collective impact of multiple regulations and allow cost recoupment.\n",
       "- Standards and Certification: [1] ABA calls for clear standards and certification processes for qualified industry standards (QIS) to ensure practical operationalization and compliance.\n",
       "- Simultaneous Rulemaking Activities: [1] ABA cautions against conducting simultaneous rulemaking activities on foundational matters, which could lead to unintended consequences.\n",
       "- Data Provider Obligations and Compliance Timeframes: [1] ABA requests clarity on data provider obligations, compliance timeframes, and the need for reasonable deadlines to ensure successful implementation.\n",
       "- Exemptions for 'Natural' Third Parties: [1] ABA recommends that 'natural' third parties, such as agents and attorneys, should be exempt from the developer interface requirements under Section 1033.\n",
       "- Electronic and Wet Signatures: [1] ABA suggests that the final rule should align with ESIGN and remove the option for wet signatures to ensure consistency with electronic data access.\n",
       "- Secondary Language Section Clarity: [1] ABA members seek additional clarity on how UDAAP will apply to the provision of financial products and services, particularly regarding the secondary language section in authorization disclosures.\n",
       "- Data Deletion Requirements: [1] ABA believes that third parties should have explicit requirements to delete data once authorization lapses or is revoked, and consumers should be able to close accounts and have data deleted upon request.\n",
       "- Retention Obligations: [1] ABA finds the retention provisions in the NPRM to be overly formalized and onerous, suggesting a reduction in the retention period to no more than 24 months and clearer guidelines on what information needs to be retained."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#display one Aggregated themes of individual comment themes\n",
    "blob_client = output_container_client.get_blob_client('aggregated_summary/'+comment_folder_name+'/overall_themes.md')\n",
    "downloaded_blob = blob_client.download_blob().readall()\n",
    "markdown_text = downloaded_blob.decode('utf-8')\n",
    "display(Markdown(markdown_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "gather": {
     "logged": 1731777323218
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Executive Summary Report\n",
       "\n",
       "#### 1. Introduction\n",
       "This executive summary consolidates feedback and comments from various stakeholders regarding the Consumer Financial Protection Bureau's (CFPB) Proposed Rule on Personal Financial Data Rights. The feedback primarily comes from financial institutions, industry associations, and other relevant organizations. The report aims to provide a comprehensive overview of the main themes, recurring issues, positive feedback, and suggestions for improvement.\n",
       "\n",
       "#### 2. Main Themes and Topics\n",
       "The feedback highlights several core themes and topics:\n",
       "- **General Support and Appreciation**: There is broad support for the CFPB's efforts to enhance consumer financial data rights, promote innovation, competition, and consumer protection.\n",
       "- **Scope and Definitions**: Concerns about the rule's scope, definitions, and limitations on data use are prevalent, with calls for refinement to avoid unintended consequences.\n",
       "- **Liability and Compliance**: Clear liability frameworks and practical compliance timelines are emphasized, with a focus on fair apportionment of liability within the financial data sharing ecosystem.\n",
       "- **Data Security and Standards**: The necessity for minimum data security standards, standardized data formats, and clear regulatory requirements is a recurring theme.\n",
       "- **Consumer Authorization and Authentication**: The need for clear distinctions between identity authentication and data sharing authorization is highlighted.\n",
       "- **Prohibition on Fees**: There is significant opposition to the prohibition on fees for data access, citing substantial costs and the need for fair compensation for data providers.\n",
       "- **Screen Scraping**: Support for prohibiting screen scraping once API access is available is strong, due to the associated risks with unsafe data access practices.\n",
       "- **Implementation and Compliance Burdens**: Concerns about the technological and financial burdens on small banks and community banks are raised, with suggestions for exemptions or fee allowances.\n",
       "\n",
       "#### 3. Recurring Issues and Concerns\n",
       "Several recurring issues and concerns are identified:\n",
       "- **Compliance Timelines**: The proposed compliance timelines are deemed insufficient, with suggestions to extend the first compliance date to 24 months after the final rule's publication.\n",
       "- **Data Security Standards**: All entities in the data sharing ecosystem should meet minimum data security standards to ensure a high level of consumer data protection.\n",
       "- **Standardized Data Formats**: Adoption of standardized data formats is supported to promote competition and reduce switching costs for third parties.\n",
       "- **Risk Management**: Allowing data providers to conduct comprehensive risk management programs without being constrained by narrow definitions of risk management concerns is emphasized.\n",
       "- **Consumer Data Protection**: Existing regulations are considered sufficient for consumer data protection, with the proposed rules not offering additional benefits.\n",
       "- **Role of Data Providers and Aggregators**: Clear regulatory obligations and oversight for data aggregators are called for, along with clarification that data providers are not responsible for ensuring third parties' compliance.\n",
       "\n",
       "#### 4. Positive Feedback\n",
       "Positive feedback includes:\n",
       "- **Consumer Empowerment and Innovation**: Emphasis on consumer control over financial data and the role of data access platforms in promoting competition and innovation.\n",
       "- **Standardization and Interoperability**: Broad support for standardized data formats and interoperable standards to facilitate secure and efficient data sharing.\n",
       "- **Regulatory Clarity and Safe Harbors**: The need for clear regulatory requirements, safe harbors for compliance, and practical approaches to authorization and data security is highlighted.\n",
       "- **Collaboration and Supervision**: Suggestions for collaboration with other regulatory bodies and direct supervision of data access platforms to ensure effective implementation and enforcement.\n",
       "\n",
       "#### 5. Suggestions for Improvement\n",
       "Key suggestions for improvement include:\n",
       "- **Extension and Flexibility in Compliance Timelines**: Extending the comment period and compliance timelines, particularly for small banks and large institutions.\n",
       "- **Clarification of Roles and Definitions**: Clear definitions of data providers, data aggregators, and third parties, as well as their respective roles and responsibilities.\n",
       "- **Prohibition of Screen Scraping and Credential-Based Access**: Explicit prohibition of screen scraping and credential-based access by third parties once API access is available.\n",
       "- **Data Security and Privacy Standards**: Robust data security standards, including minimum data security requirements, tokenized account numbers, and clear data deletion and retention policies.\n",
       "- **Supervision and Certification of Third Parties**: Creating a supervisory program for data aggregators, establishing a non-binding accreditation body for nonbanks, and requiring third parties to become authorized and certified.\n",
       "- **Consumer Control and Transparency**: Enhancing consumer control over their data with opt-in/opt-out choices, clear revocation methods, and transparency in data access and usage.\n",
       "- **Fee Structures and Cost Recovery**: Allowing data providers to charge reasonable fees for data access and developer interfaces, and to recover costs and margins.\n",
       "- **Performance and Reporting Standards**: Adopting commercially reasonable performance specifications, modifying latency thresholds, and allowing industry bodies to publish performance statistics.\n",
       "- **Regulatory Coordination and Avoidance of Duplication**: Coordinating with other regulatory bodies, avoiding simultaneous rulemaking activities, and ensuring new rules do not duplicate existing regulatory obligations.\n",
       "- **Secondary Data Use and Data Sharing Limitations**: Allowing responsible secondary use of data with appropriate safeguards, exempting de-identified data from restrictions, and establishing stringent limitations on secondary data sharing.\n",
       "\n",
       "#### 6. Conclusion\n",
       "The feedback on the CFPB's Proposed Rule on Personal Financial Data Rights underscores the need for a balanced approach that ensures consumer protection, promotes innovation, and provides clear and practical regulatory requirements. While there is broad support for the CFPB's efforts, stakeholders advocate for refinements to address their concerns and enhance the rule's effectiveness. Key areas for improvement include extending compliance timelines, clarifying roles and definitions, enhancing data security and privacy standards, and ensuring consumer control and transparency. By addressing these suggestions, the CFPB can create a more effective and balanced regulatory framework for personal financial data rights."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#display executive summary based on aggregated summary, suggestions and themes\n",
    "blob_client = output_container_client.get_blob_client('final/'+comment_folder_name+'/exec_summary.md')\n",
    "downloaded_blob = blob_client.download_blob().readall()\n",
    "markdown_text = downloaded_blob.decode('utf-8')\n",
    "display(Markdown(markdown_text))"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Profile March\n",
        "![Profile Match Approach](../Other/Profile_Match_Flow.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Prerequisites\n",
        "* Convert PowerPoint to JPG(PPTX --> Save AS --> Select JPG option and specify a folder to save all the images of the slides). This will save each slide as seperate image files (Slide1.JPG, Slide2.JPG etc).\n",
        "* Create model deployments for GPT-4o,  dall-e-3 and text-embedding-3-large models\n",
        "* Update the local.env file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%pip install --force-reinstall -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1735941997684
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import AzureOpenAI\n",
        "import re\n",
        "from azure.storage.blob import (\n",
        "    BlobServiceClient,\n",
        "    BlobClient,\n",
        "    BlobSasPermissions,\n",
        "    generate_blob_sas,\n",
        "    ContainerClient,\n",
        ")\n",
        "from datetime import datetime, timedelta, timezone\n",
        "\n",
        "import json\n",
        "import requests\n",
        "import math\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"local.env\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Helper Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1735942063752
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_sas_token(\n",
        "    blob_service_client: BlobServiceClient, source_blob: BlobClient\n",
        ") -> str:\n",
        "\n",
        "    # Create a SAS token that's valid for one hour, as an example\n",
        "    sas_token = generate_blob_sas(\n",
        "        account_name=blob_service_client.account_name,\n",
        "        container_name=source_blob.container_name,\n",
        "        blob_name=source_blob.blob_name,\n",
        "        account_key=blob_service_client.credential.account_key,\n",
        "        permission=BlobSasPermissions(read=True),\n",
        "        expiry=datetime.now(timezone.utc) + timedelta(hours=1),\n",
        "        start=datetime.now(timezone.utc) + timedelta(hours=-1),\n",
        "    )\n",
        "    return sas_token\n",
        "\n",
        "\n",
        "\n",
        "def generate_embeddings(text, model): \n",
        "    return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
        "\n",
        "#just a dot product should be OK too\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "def getResponseFromAoAi_image(systemPrompt, userPrompt, temp, imageurl):\n",
        "    temperature = temp if temp else 0\n",
        "    conversaion = [\n",
        "        {\"role\": \"system\", \"content\": systemPrompt},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": userPrompt},\n",
        "                {\"type\": \"image_url\", \"image_url\": {\"url\": imageurl}},\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "    # Send the conversation to the API\n",
        "    response = client.chat.completions.create(\n",
        "        model=aoai_api_deployment_name,  # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
        "        messages=conversaion,\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    # Print the assistant's response\n",
        "    responseText = response.choices[0].message.content\n",
        "    return responseText\n",
        "\n",
        "\n",
        "def getResponseFromAoAi(systemPrompt, userPrompt, temp):\n",
        "    temperature = temp if temp else 0\n",
        "    conversaion = [\n",
        "        {\"role\": \"system\", \"content\": systemPrompt},\n",
        "        {\"role\": \"user\", \"content\": userPrompt},\n",
        "    ]\n",
        "    # Send the conversation to the API\n",
        "    response = client.chat.completions.create(\n",
        "        model=aoai_api_deployment_name,  # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
        "        messages=conversaion,\n",
        "        # response_format={ \"type\": \"json_object\" }, #requires ptu enabled gpt4\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    # Print the assistant's response\n",
        "    responseText = response.choices[0].message.content\n",
        "    return responseText\n",
        "\n",
        "\n",
        "def clean_json_string(json_string):\n",
        "    pattern = r\"^```json\\s*(.*?)\\s*```$\"\n",
        "    cleaned_string = re.sub(pattern, r\"\\1\", json_string, flags=re.DOTALL)\n",
        "    return cleaned_string.strip()\n",
        "\n",
        "\n",
        "def merge_json_files(container_client: ContainerClient, profile_folder_name: str):\n",
        "    try:\n",
        "        combined_data = []\n",
        "        blob_list = container_client.list_blobs(\n",
        "            name_starts_with=output_folder_name\n",
        "            + \"/\"\n",
        "            + profile_text_folder_name\n",
        "            + \"/\"\n",
        "            + profile_folder_name\n",
        "        )\n",
        "        for blob in blob_list:\n",
        "            if blob.name.endswith(\".json\"):\n",
        "                blob_client = container_client.get_blob_client(blob.name)\n",
        "                downloaded_blob = blob_client.download_blob().readall()\n",
        "                json_data = json.loads(downloaded_blob)\n",
        "                combined_data.append(json_data)\n",
        "\n",
        "        agg_team_profiles_blob_client = container_client.get_blob_client(\n",
        "            output_folder_name\n",
        "            + \"/\"\n",
        "            + profile_aggregated_folder_name\n",
        "            + \"/\"\n",
        "            + profile_folder_name\n",
        "            + \"/aggregated_team_profiles.json\"\n",
        "        )\n",
        "        agg_team_profiles_blob_client.upload_blob(\n",
        "            json.dumps(combined_data, indent=4), overwrite=True\n",
        "        )\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to generate aggregated team_profiles: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def generate_profile_images(\n",
        "    container_client: ContainerClient, profile_folder_name: str\n",
        "):\n",
        "    try:\n",
        "        print(\"Started image creating process\")\n",
        "        blob_list = container_client.list_blobs(\n",
        "            name_starts_with=output_folder_name\n",
        "            + \"/\"\n",
        "            + profile_text_folder_name\n",
        "            + \"/\"\n",
        "            + profile_folder_name\n",
        "        )\n",
        "        for blob in blob_list:\n",
        "            try:\n",
        "                print(\"\\t\" + blob.name)\n",
        "                print(\"type:\", blob.content_settings.content_type)\n",
        "                if blob.name.endswith(\".json\"):\n",
        "                    blob_client = container_client.get_blob_client(blob.name)\n",
        "                    downloaded_blob = blob_client.download_blob().readall()\n",
        "                    json_data = json.loads(downloaded_blob)\n",
        "                    final_image_prompt = img_profile_prompt.format(\n",
        "                        person_profile_summary=json_data[\"Overall_Summary\"]\n",
        "                    )\n",
        "                    print(final_image_prompt)\n",
        "\n",
        "                    result = client.images.generate(\n",
        "                        model=\"Dalle3\",  # the name of your DALL-E 3 deployment\n",
        "                        prompt=final_image_prompt,\n",
        "                        n=1,\n",
        "                    )\n",
        "                    # image_url = json.loads(result.model_dump_json())['data'][0]['url']\n",
        "                    json_response = json.loads(result.model_dump_json())\n",
        "                    # Retrieve the generated image\n",
        "                    image_url = json_response[\"data\"][0][\"url\"]\n",
        "                    generated_image = requests.get(image_url).content\n",
        "                    img_file_name = blob.name.replace(\n",
        "                        output_folder_name + \"/\" + profile_text_folder_name,\n",
        "                        output_folder_name + \"/\" + profile_img_folder_name,\n",
        "                    ).replace(\".json\", \".jpg\")\n",
        "                    print(img_file_name)\n",
        "                    img_blob_client = container_client.get_blob_client(img_file_name)\n",
        "                    img_blob_client.upload_blob(\n",
        "                        generated_image, blob_type=\"BlockBlob\", overwrite=True\n",
        "                    )\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred with item {blob.name}: {e}\")\n",
        "                print(\"eroor code:\", e.code)\n",
        "                # Continue to the next item in the loop\n",
        "                continue\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to generate profile images: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def match_profiles(\n",
        "    container_client: ContainerClient, temperature: float, profile_folder_name: str\n",
        "):\n",
        "    try:\n",
        "        blob_client = container_client.get_blob_client(\n",
        "            output_folder_name\n",
        "            + \"/\"\n",
        "            + profile_aggregated_folder_name\n",
        "            + \"/\"\n",
        "            + profile_folder_name\n",
        "            + \"/aggregated_team_profiles.json\"\n",
        "        )\n",
        "\n",
        "        downloaded_blob = blob_client.download_blob().readall()\n",
        "        json_data = json.loads(downloaded_blob)\n",
        "        updated_json = [\n",
        "            {\n",
        "                \"Name\": item[\"Team_Member_Name\"],\n",
        "                \"What_I_Do\": item[\"What_I_Do\"],\n",
        "                \"What_I_Like\": item[\"What_I_Like\"],\n",
        "                \"Overall_Summary\": item[\"Overall_Summary\"],\n",
        "            }\n",
        "            for item in json_data\n",
        "        ]\n",
        "\n",
        "        for i in range(len(updated_json)):\n",
        "            try:\n",
        "                current_object = updated_json[i]  # Object in the current loop\n",
        "                rest_of_objects = (\n",
        "                    updated_json[:i] + updated_json[i + 1 :]\n",
        "                )  # JSON for the rest of the objects\n",
        "                name_profile = json.dumps(current_object[\"Name\"]).strip('\"')\n",
        "                print(\"Current Object:\", name_profile)\n",
        "                print(\"\\n\")\n",
        "                part_length = math.ceil(len(rest_of_objects) / 10)\n",
        "                # Split the item list into the specified number of parts\n",
        "                parts = [\n",
        "                    rest_of_objects[i : i + part_length]\n",
        "                    for i in range(0, len(rest_of_objects), part_length)\n",
        "                ]\n",
        "                final_json = []\n",
        "                for part in parts:\n",
        "                    final_user_prompt = member_match_usr_prompt.format(\n",
        "                        team_member_profile=json.dumps(current_object, indent=4),\n",
        "                        rest_of_team_profiles=json.dumps(part, indent=4),\n",
        "                    )\n",
        "                    aoai_response = getResponseFromAoAi(\n",
        "                        member_match_sys_prompt, final_user_prompt, temp=temperature\n",
        "                    )\n",
        "                    cleaned_response = clean_json_string(aoai_response)\n",
        "                    parsed_json = json.loads(cleaned_response)\n",
        "                    final_json.extend(parsed_json)\n",
        "                sorted_json_data = sorted(\n",
        "                    final_json,\n",
        "                    key=lambda x: x[\"Overall_Similarity_Score\"],\n",
        "                    reverse=True,\n",
        "                )\n",
        "\n",
        "                blob_client = container_client.get_blob_client(\n",
        "                    output_folder_name\n",
        "                    + \"/\"\n",
        "                    + profile_matches_folder_name\n",
        "                    + \"/\"\n",
        "                    + profile_folder_name\n",
        "                    + \"/\"\n",
        "                    + name_profile\n",
        "                    + \".json\"\n",
        "                )\n",
        "                blob_client.upload_blob(\n",
        "                    json.dumps(sorted_json_data, indent=4), overwrite=True\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(\n",
        "                    f\"An error occurred while generating matches for profile {name_profile}: {e}\"\n",
        "                )\n",
        "                print(\"eroor code:\", e.code)\n",
        "                # Continue to the next item in the loop\n",
        "                continue\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Failed while matching profiles: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1735942066138
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "image_insights_sys_prompt = \"\"\" \n",
        "You are a specialized AI model designed to extract and analyze text and visual information from images containing team member profiles. Your task is to process each image, identify and categorize the textual information under predefined topics, and provide a description and summary of the images. The predefined topics are \"Who I Am,\" \"What I Do,\" \"What I Like,\" \"Analyze Me,\" \"The Last Word,\" ,  \"To Work With Me, You Should Know\",\"Career Path & Education\", \"Passions\", \"I Thrive On\" and \"I'm Good @\".   Note that not all images may contain information on every topic. Your primary goal is to capture detailed information that will be used to match people based on their interests, hobbies, skills, experience, and other relevant attributes. The images contain personal and professional details, along with pictures representing their interests and lifestyle.\n",
        "\n",
        "For each image, follow these steps:\n",
        "\n",
        "1. **Extract Team Member's Name:**\n",
        "   - Identify and extract the name of the team member to whom the image belongs.\n",
        "   - This name should be placed at the top of the JSON output as a separate element.\n",
        "\n",
        "2. **Extract Text:**\n",
        "   - Identify and extract text related to the topics listed above.\n",
        "   - If a topic is not mentioned in the image, return a \"Not Provided\" status for that topic.\n",
        "   - Based on the information from extracted text, identify the current location (City and state), if available.\n",
        "   - Extract any additional relevant information that could help in matching people, such as certifications, languages spoken, or specific project experience.\n",
        "\n",
        "3. **Image Analysis:**\n",
        "   - Describe the content of the pictures with a focus on extracting every possible detail.\n",
        "   - Pay attention to all elements such as people, objects, food, places, activities, and settings that reflect the individual's personal or professional life.\n",
        "   - Ensure to capture details like the food the person is eating, the places they are in, clothing, and any background elements that might provide insights into their lifestyle, interests, or personality.\n",
        "   - Extract all details from the pictures and ensure no detail is missed.\n",
        "\n",
        "4. **Identify Superhero Match:**\n",
        "   - Based on the extracted profile information, identify a superhero that closely matches the person’s characteristics, personality, or lifestyle.\n",
        "   - Provide a brief reason for this match.\n",
        "\n",
        "5. **Identify Food Recommendation:**\n",
        "   - Based on the extracted profile information, suggest a food item that the person should try.\n",
        "   - Provide a brief explanation for the recommendation.\n",
        "\n",
        "6. **Detailed Insights Based on Pictures:**\n",
        "   - Analyze the pictures mentioned in the image and provide detailed insights about the person.\n",
        "   - These insights should be based on the content of the pictures and reflect the individual’s personality, interests, lifestyle, and any other relevant attributes.\n",
        "\n",
        "7. **Generate JSON:**\n",
        "   - Organize the extracted information into the following structured JSON format.Ensure accuracy in filling out each field based on the information extracted from the image. Handle missing data with \"Not Provided\" for the predefined topics or leave optional fields empty if they do not apply.\n",
        "{\n",
        "  \"Team_Member_Name\": \"team member name\",\n",
        "  \"Who_I_Am\": \"details extracted related to who I am and Career Path & Education\",\n",
        "  \"What_I_Do\": \"details extracted related to what I do\",\n",
        "  \"What_I_Like\": \"details extracted related to what I like and Passions\",\n",
        "  \"Analyze_Me\": \"details extracted related to analyze me\",\n",
        "  \"The_Last_Word\": \"details extracted related to the last word\",\n",
        "  \"Other Info\":\"details extracted related to I'm Good at and I thrive on\", \n",
        "  \"Current_Location\": \"details extracted related to current location\",,\n",
        "  \"To_Work_With_Me_You_Should_Know\": \"details extracted related to who I am\",\n",
        "  \"Image_Analysis\": \"all details captured from the pictures in the image, including food, places, and other elements\",\n",
        "  \"Superhero_Match\": {\n",
        "    \"Superhero\": \"superhero identified\",\n",
        "    \"Reason\": \"brief reason for the match\"\n",
        "  },\n",
        "  \"Food_Recommendation\": {\n",
        "    \"Food_Item\": \"recommended food item\",\n",
        "    \"Reason\": \"brief reason for the recommendation\"\n",
        "  },\n",
        "  \"Detailed_Insights_From_Pictures\": \"detailed insights about the person based on the pictures mentioned in the image\",\n",
        "  \"Overall_Summary\": \"detailed summary combining all of the above information for an overall understanding of the team member’s profile. Make sure to include all of the details extracted including Superhero_Match and Food_Recommendation if available.\"\n",
        "}\n",
        "\n",
        "8. **Considerations:**\n",
        "   - Ensure accuracy in extracting text, especially when dealing with varied fonts and layouts.\n",
        "   - Provide clear and concise descriptions for images that adequately represent the visual content.\n",
        "   - Be mindful of incomplete information, and appropriately handle missing data without making assumptions.\n",
        "\n",
        "9. **Output Requirements:**\n",
        "   - Produce the final output in a proper JSON format. The JSON should be clean and properly structured without any additional markers or formatting such as triple backticks or other delimiters.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "image_insights_usr_prompt = \"\"\"\n",
        "Extract all the relevant information from the attached image file:\n",
        "\n",
        "Please, make sure that the output is valid JSON. \n",
        "\"\"\"\n",
        "\n",
        "\n",
        "member_match_sys_prompt = \"\"\"\n",
        "You are an AI model tasked with assigning similarity scores between a given team member's profile and the profiles of other team members based on their interests, skills, and personality. You will be provided with a JSON object containing profiles of all team members.\n",
        "\n",
        "Your task is to:\n",
        "\n",
        "1. **Evaluate All Profiles and Assign Similarity Scores**:\n",
        "   - For each profile in the JSON, evaluate its similarity to the given team member's profile based on the following criteria:\n",
        "     - **Interests**: From the `What_I_Like` section. Assign a similarity score from 1 to 10, where 10 represents very high similarity.\n",
        "     - **Skills and Experience**: From the `What_I_Do` section. Assign a similarity score from 1 to 10, where 10 represents very high similarity.\n",
        "     - **Personality**: From the `Overall_Summary` and any relevant information in other sections. Assign a similarity score from 1 to 10, where 10 represents very high similarity.\n",
        "   - Calculate an **overall similarity score** for each profile by averaging the individual similarity scores for interests, skills, and personality.\n",
        "\n",
        "2. **Generate a JSON Output**:\n",
        "   - The output should consist of valid JSON only, with no extra text or explanations. Each profile's output should include:\n",
        "     - `Name`: Name of the team member.\n",
        "     - `Interests_Similarity_Score`: The similarity score for interests (1 to 10).\n",
        "     - `Skills_Similarity_Score`: The similarity score for skills and experience (1 to 10).\n",
        "     - `Personality_Similarity_Score`: The similarity score for personality (1 to 10).\n",
        "     - `Overall_Similarity_Score`: The overall similarity score for this profile, calculated as the average of the three individual scores.\n",
        "\n",
        "3. **Important Instruction**:\n",
        "   - Respond **only** with the JSON output in the format specified.\n",
        "   - Do not include any other text or explanations.\n",
        "   - Ensure that the response is a valid JSON format.\n",
        "\n",
        "### JSON Output Example:\n",
        "```json\n",
        "[\n",
        "  {\n",
        "    \"Name\": \"Adam Davenport\",\n",
        "    \"Interests_Similarity_Score\": 1,\n",
        "    \"Skills_Similarity_Score\": 5,\n",
        "    \"Personality_Similarity_Score\": 18,\n",
        "    \"Overall_Similarity_Score\": 8\n",
        "  },\n",
        "  {\n",
        "    \"Name\": \"Emily Brown\",\n",
        "    \"Interests_Similarity_Score\": 19,\n",
        "    \"Skills_Similarity_Score\": 4,\n",
        "    \"Personality_Similarity_Score\": 4,\n",
        "    \"Overall_Similarity_Score\": 9\n",
        "  }\n",
        "]\n",
        "\n",
        "\"\"\"\n",
        "member_match_usr_prompt = \"\"\"\n",
        "Here is JSON for team_member_profile:{team_member_profile}\n",
        "\n",
        "Here is JSON for rest_of_team_profiles :{rest_of_team_profiles}\n",
        "\n",
        "Generate JSON Output as mentioned above. \n",
        "\"\"\"\n",
        "\n",
        "img_profile_prompt = \"\"\"                \n",
        "    Create a 4-section image representing an individual based on the following summary: \n",
        "    {person_profile_summary}\n",
        "    Each section should depict this person in a different context, such as their hobbies, interests, likes, dislikes, and positive personality traits. Ensure each section is visually distinct and captures the essence of their diverse characteristics in a respectful and creative manner. No text or religious symbols, imagery, or elements should be included in the image. Make sure to  take their Superhero match wwhile creating images. \n",
        "        \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Initialize the variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1735942081749
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "aoai_api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
        "aoai_api_endpoint =  os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        "aoai_api_deployment_name =  os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
        "dalle_deployment = os.getenv(\"AZURE_DALLE_DEPLOYMENT_NAME\")\n",
        "embeding_model = os.getenv(\"AZURE_EMBEDDING_MODEL\")\n",
        "account_name = os.getenv(\"STORAGE_ACCOUNT_NAME\")\n",
        "account_key = os.getenv(\"STORAGE_ACCOUNT_KEY\")\n",
        "input_container_name = os.getenv(\"STORAGE_CONTAINER_NAME\")\n",
        "connection_string = f\"DefaultEndpointsProtocol=https;AccountName={account_name};AccountKey={account_key};EndpointSuffix=core.windows.net\"\n",
        "aoai_api_version= '2024-06-01'\n",
        "\n",
        "input_folder_name = \"input\"\n",
        "output_folder_name = \"output\"\n",
        "profile_text_folder_name = \"profile_text\"\n",
        "profile_img_folder_name = \"profile_img\"\n",
        "profile_aggregated_folder_name = \"profiles_aggregated\"\n",
        "profile_matches_folder_name = \"profiles_matches\"\n",
        "\n",
        "chunk_tokens = 8000\n",
        "profile_folder_name = \"test2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Extract the profile info and generate JSON file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1735942112644
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    print(\"starting time:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "    \n",
        "    # define openai and document intelligence Clients\n",
        "    client = AzureOpenAI(\n",
        "        api_key=aoai_api_key, api_version=aoai_api_version, azure_endpoint=aoai_api_endpoint\n",
        "    )\n",
        "\n",
        "    # Create a BlobServiceClient object using the connection string\n",
        "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
        "\n",
        "    # Create a ContainerClient object\n",
        "    input_container_client = blob_service_client.get_container_client(\n",
        "        input_container_name\n",
        "    )\n",
        "\n",
        "\n",
        "    # List the blobs in the container\n",
        "    blob_list = input_container_client.list_blobs(name_starts_with=input_folder_name+\"/\"+profile_folder_name)\n",
        "    chunks = []\n",
        "    team_file = False\n",
        "\n",
        "    print(\"Started profile info extraction process\")\n",
        "    # extract profile info and store as json\n",
        "    for blob in blob_list:\n",
        "        print(\"\\t\" + blob.name)\n",
        "        print(\"type:\", blob.content_settings.content_type)\n",
        "        blob_client = input_container_client.get_blob_client(blob.name)\n",
        "        sas_token = generate_sas_token(\n",
        "            blob_service_client=blob_service_client, source_blob=blob_client\n",
        "        )\n",
        "        source_blob_sas_url = blob_client.url + \"?\" + sas_token\n",
        "        llm_response_file_name = \"\"\n",
        "\n",
        "        if blob.content_settings.content_type == \"image/jpeg\":\n",
        "            team_file = True\n",
        "            aoai_response = getResponseFromAoAi_image(\n",
        "                image_insights_sys_prompt,\n",
        "                image_insights_usr_prompt,\n",
        "                0.7,\n",
        "                source_blob_sas_url,\n",
        "            )\n",
        "            cleaned_response = clean_json_string(aoai_response)\n",
        "            json_data = json.loads(cleaned_response)\n",
        "            profile_name = json_data.get(\"Team_Member_Name\").strip('\"')\n",
        "            folder_path= os.path.dirname(blob.name).replace(input_folder_name, \"\")\n",
        "            llm_response_file_name = output_folder_name+ \"/\"+profile_text_folder_name+folder_path+\"/\"+profile_name+\".json\"\n",
        "\n",
        "            blob_client = input_container_client.get_blob_client(\n",
        "                llm_response_file_name\n",
        "            )\n",
        "            blob_client.upload_blob(cleaned_response, overwrite=True)\n",
        "\n",
        "    \n",
        "\n",
        "    print(\"End time:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "\n",
        "except Exception as ex:\n",
        "    print(\"Exception:\")\n",
        "    print(ex)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Generate the following\n",
        "* Profile Images\n",
        "* Aggregated Profile JSON file\n",
        "* Identify Matches for each profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1735942200654
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "if team_file:\n",
        "    if generate_profile_images(input_container_client, profile_folder_name):\n",
        "        print(\"Successfully generated profile images\")\n",
        "    else:\n",
        "        print(\"Failed to generate profile images\")\n",
        "  \n",
        "    if( merge_json_files(input_container_client , profile_folder_name) ):\n",
        "        print(\"Successfully merged profiles\")\n",
        "    else:\n",
        "        print(\"Failed to merge profiles\")  \n",
        "\n",
        "    if( match_profiles(input_container_client , 0.0, profile_folder_name) ):\n",
        "        print(\"Successfully matched profiles\")\n",
        "    else:\n",
        "        print(\"Failed to match profiles\")"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
